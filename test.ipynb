{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyreadstat\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Paths to input files and output directories\n",
    "current_directory = os.getcwd()\n",
    "sav_file_path = os.path.join(current_directory, '01-data', 'yaleoutput.sav')\n",
    "rename_excel_path = os.path.join(current_directory, '01-data', 'column_rename_mapping.xlsx')\n",
    "output_excel_path = os.path.join(current_directory, '03-output', 'dfyale.xlsx')\n",
    "\n",
    "# Read the .sav file\n",
    "df, meta = pyreadstat.read_sav(sav_file_path)\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['starttime', 'endtime']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Load the rename mapping from the Excel file\n",
    "rename_df = pd.read_excel(rename_excel_path, sheet_name=\"Mapping\")\n",
    "column_rename_mapping = dict(zip(rename_df['OldName'], rename_df['NewName']))\n",
    "\n",
    "# Rename the columns\n",
    "df.rename(columns=column_rename_mapping, inplace=True)\n",
    "\n",
    "# Drop columns ending in '_select'\n",
    "select_columns = [col for col in df.columns if col.endswith('_select')]\n",
    "df.drop(columns=select_columns, inplace=True)\n",
    "\n",
    "# Get the current year\n",
    "current_year = datetime.now().year\n",
    "\n",
    "# Compute ages based on the current year\n",
    "df['age'] = current_year - df['birthyr']\n",
    "\n",
    "# Define age group bins with \"65+\" as the last group\n",
    "bins = [0, 17, 24, 34, 44, 54, 64, 100]\n",
    "labels = ['0-17', '18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "\n",
    "# Categorize ages into the specified groups\n",
    "df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Convert all numerical columns to floats and round to one decimal place\n",
    "for col in df.select_dtypes(include=['number']).columns:\n",
    "    df[col] = df[col].astype(float).round(1)\n",
    "\n",
    "# Load the grouping data from the Excel file\n",
    "grouping_df = pd.read_excel(rename_excel_path, sheet_name=\"Grouping\")\n",
    "\n",
    "# Mapping functions by suffix\n",
    "suffix_mappings = {\n",
    "    '_impact': dict(zip(grouping_df[grouping_df['Suffix'] == '_impact']['Code'], grouping_df[grouping_df['Suffix'] == '_impact']['Recode'])),\n",
    "    '_confidence': dict(zip(grouping_df[grouping_df['Suffix'] == '_confidence']['Code'], grouping_df[grouping_df['Suffix'] == '_confidence']['Recode'])),\n",
    "    '_important': dict(zip(grouping_df[grouping_df['Suffix'] == '_important']['Code'], grouping_df[grouping_df['Suffix'] == '_important']['Recode'])),\n",
    "    '_likelihood': dict(zip(grouping_df[grouping_df['Suffix'] == '_likelihood']['Code'], grouping_df[grouping_df['Suffix'] == '_likelihood']['Recode'])),\n",
    "    '_agreement': dict(zip(grouping_df[grouping_df['Suffix'] == '_agreement']['Code'], grouping_df[grouping_df['Suffix'] == '_agreement']['Recode'])),\n",
    "    'gender': dict(zip(grouping_df[grouping_df['Suffix'] == 'gender']['Code'], grouping_df[grouping_df['Suffix'] == 'gender']['Recode'])),\n",
    "    'hhi': dict(zip(grouping_df[grouping_df['Suffix'] == 'hhi']['Code'], grouping_df[grouping_df['Suffix'] == 'hhi']['Recode'])),\n",
    "    'marstat': dict(zip(grouping_df[grouping_df['Suffix'] == 'marstat']['Code'], grouping_df[grouping_df['Suffix'] == 'marstat']['Recode'])),\n",
    "    'educ': dict(zip(grouping_df[grouping_df['Suffix'] == 'educ']['Code'], grouping_df[grouping_df['Suffix'] == 'educ']['Recode'])),\n",
    "    'race': dict(zip(grouping_df[grouping_df['Suffix'] == 'race']['Code'], grouping_df[grouping_df['Suffix'] == 'race']['Recode'])),\n",
    "    'state': dict(zip(grouping_df[grouping_df['Suffix'] == 'state']['Code'], grouping_df[grouping_df['Suffix'] == 'state']['Recode'])),\n",
    "    'employ': dict(zip(grouping_df[grouping_df['Suffix'] == 'employ']['Code'], grouping_df[grouping_df['Suffix'] == 'employ']['Recode'])),\n",
    "    'Xvar_politics': dict(zip(grouping_df[grouping_df['Suffix'] == 'Xvar_politics']['Code'], grouping_df[grouping_df['Suffix'] == 'Xvar_politics']['Recode'])),\n",
    "    'Xvar_Q04_R01_ed_courses_CS': dict(zip(grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R01_ed_courses_CS']['Code'], grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R01_ed_courses_CS']['Recode'])),\n",
    "    'Xvar_Q04_R02_ed_undergrad_CS': dict(zip(grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R02_ed_undergrad_CS']['Code'], grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R02_ed_undergrad_CS']['Recode'])),\n",
    "    'Xvar_Q04_R03_ed_Grad_CS': dict(zip(grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R03_ed_Grad_CS']['Code'], grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R03_ed_Grad_CS']['Recode'])),\n",
    "    'Xvar_Q04_R04_ed_program_exp': dict(zip(grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R04_ed_program_exp']['Code'], grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R04_ed_program_exp']['Recode'])),\n",
    "    'Xvar_Q04_R05_ed_none': dict(zip(grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R05_ed_none']['Code'], grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R05_ed_none']['Recode'])),\n",
    "    'Y_Q05_SupportAI': dict(zip(grouping_df[grouping_df['Suffix'] == 'Y_Q05_SupportAI']['Code'], grouping_df[grouping_df['Suffix'] == 'Y_Q05_SupportAI']['Recode'])),\n",
    "    'Y_Q17_SupportDevHighlevelAI': dict(zip(grouping_df[grouping_df['Suffix'] == 'Y_Q17_SupportDevHighlevelAI']['Code'], grouping_df[grouping_df['Suffix'] == 'Y_Q17_SupportDevHighlevelAI']['Recode']))\n",
    "}\n",
    "\n",
    "# Function to recode columns based on suffix\n",
    "def apply_mapping(df, mapping_dict, suffix):\n",
    "    for col in df.columns:\n",
    "        if col.endswith(suffix):\n",
    "            df[col] = df[col].replace(mapping_dict)\n",
    "\n",
    "# Apply mappings for each suffix\n",
    "for suffix, mapping_dict in suffix_mappings.items():\n",
    "    apply_mapping(df, mapping_dict, suffix)\n",
    "\n",
    "# List of columns to be one-hot encoded\n",
    "columns_to_encode = ['Xvar_politics', 'Xvar_Q04_R01_ed_courses_CS', 'Xvar_Q04_R02_ed_undergrad_CS', 'Xvar_Q04_R03_ed_Grad_CS', 'Xvar_Q04_R04_ed_program_exp', 'Xvar_Q04_R05_ed_none']\n",
    "\n",
    "# Apply one-hot encoding to categorical columns, which will use 0 and 1\n",
    "df_dummies = pd.get_dummies(df, columns=columns_to_encode, drop_first=False)\n",
    "\n",
    "# Convert all Boolean columns to 0/1\n",
    "bool_columns = df_dummies.select_dtypes(include=['bool']).columns\n",
    "df_dummies[bool_columns] = df_dummies[bool_columns].astype(int)\n",
    "\n",
    "# Drop unwanted columns\n",
    "columns_to_drop = ['birthyr', 'pid7', 'votereg', 'ideo5', 'newsint', 'religpew', 'pew_churatd', 'pew_bornagain', 'pew_religimp', 'pew_prayer', 'Q03new_treat', 'q05b_treat', 'q12a_treat', 'q12_treat', 'q15_treat', 'Xvar_Q05b','Xvar_Q04_R05_ed_none_No','Xvar_Q04_R04_ed_program_exp_No','Xvar_Q04_R03_ed_Grad_CS_No','Xvar_Q04_R02_ed_undergrad_CS_No','Xvar_Q04_R01_ed_courses_CS_No']\n",
    "df_dummies = df_dummies.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Add the 'NA' category to all categorical columns and then fill missing values\n",
    "for col in df_dummies.select_dtypes(include='category').columns:\n",
    "    df_dummies[col] = df_dummies[col].cat.add_categories(['NA'])\n",
    "df_dummies = df_dummies.fillna('NA')\n",
    "\n",
    "weight_column = 'weight'\n",
    "\n",
    "# Identify columns starting with \"Xvar\" or \"Y_\"\n",
    "columns_to_weight = [col for col in df_dummies.columns if col.startswith('Xvar') or col.startswith('Y_')]\n",
    "\n",
    "# Ensure the columns are converted to numeric or boolean types before applying weights\n",
    "for col in columns_to_weight:\n",
    "    # Check if the column is categorical using isinstance with pd.CategoricalDtype\n",
    "    if isinstance(df_dummies[col].dtype, pd.CategoricalDtype):\n",
    "        df_dummies[col] = df_dummies[col].cat.codes\n",
    "    # Convert boolean columns to integers (0/1)\n",
    "    elif pd.api.types.is_bool_dtype(df_dummies[col]):\n",
    "        df_dummies[col] = df_dummies[col].astype(int)\n",
    "    # Convert remaining columns to numeric, coercing errors to NaN\n",
    "    df_dummies[col] = pd.to_numeric(df_dummies[col], errors='coerce')\n",
    "\n",
    "# Now apply the weight from the weight column to the identified columns\n",
    "for col in columns_to_weight:\n",
    "    df_dummies[col] = df_dummies[col] * df_dummies[weight_column]\n",
    "\n",
    "# Save the modified DataFrame to an Excel file\n",
    "df_dummies.to_excel(output_excel_path, index=False)\n",
    "\n",
    "print(f\"DataFrame with renamed columns is saved to {output_excel_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: some nodes with margin (3.20,3.20) touch - falling back to straight line edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model graph saved to: /Users/danramirez/mbs-structural-equation-modeling/04-summary/sem_model_graph_top40.pdf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from semopy import Model\n",
    "from graphviz import Digraph\n",
    "import os\n",
    "\n",
    "# Load the original data\n",
    "current_directory = os.getcwd()\n",
    "survey = os.path.join(current_directory, '03-output', 'dfyale.xlsx')\n",
    "sheet_name = 'Sheet1'\n",
    "df = pd.read_excel(survey, sheet_name=sheet_name)\n",
    "\n",
    "# Identify and sort the independent columns (features) starting with 'Xvar'\n",
    "feature_columns = [col for col in df.columns if col.startswith('Xvar')]\n",
    "feature_variances = df[feature_columns].var().sort_values(ascending=False)\n",
    "top_features = feature_variances.head(40).index\n",
    "\n",
    "# Apply Factor Analysis to reduce features\n",
    "num_components = 4\n",
    "fa = FactorAnalysis(n_components=num_components)\n",
    "fa_result = fa.fit_transform(df[top_features])\n",
    "\n",
    "# Create a DataFrame for the Factor Analysis-transformed features\n",
    "fa_columns = [f'Factor{i+1}' for i in range(num_components)]\n",
    "fa_df = pd.DataFrame(fa_result, columns=fa_columns)\n",
    "\n",
    "# Combine with dependent variables\n",
    "dependent_columns = ['Y_Q05_SupportAI']\n",
    "final_df = pd.concat([fa_df, df[dependent_columns]], axis=1)\n",
    "\n",
    "# SEM model with reduced Factor Analysis features\n",
    "model_desc = f\"\"\"\n",
    "# Structural Model\n",
    "Y_Q05_SupportAI ~ {' + '.join(fa_columns)}\n",
    "\"\"\"\n",
    "\n",
    "# Create the SEM model and load the dataset\n",
    "model = Model(model_desc)\n",
    "model.load_dataset(final_df)\n",
    "\n",
    "# Optimize the model\n",
    "model.fit()\n",
    "\n",
    "# Create a graph using the fdp engine in graphviz\n",
    "graph = Digraph(comment='SEM Model', engine='fdp', format='pdf')\n",
    "graph.attr(overlap='false', splines='true', dpi='300')\n",
    "\n",
    "# Add nodes (features) and edges (relationships) manually\n",
    "nodes = fa_columns + ['Y_Q05_SupportAI']\n",
    "for node in nodes:\n",
    "    graph.node(node, node, shape='box', fontsize='20')\n",
    "\n",
    "# Add relationships (edges) between Factor Analysis components and dependent variables\n",
    "for source in fa_columns:\n",
    "    graph.edge(source, 'Y_Q05_SupportAI', fontsize='16')\n",
    "\n",
    "# Extract Factor Analysis loadings for each factor\n",
    "loadings = pd.DataFrame(fa.components_, columns=top_features, index=fa_columns)\n",
    "\n",
    "# Visualize loadings with labels above a certain threshold for better clarity\n",
    "threshold = 0.2  # Adjust this value to show more significant edges\n",
    "for factor, features in loadings.iterrows():\n",
    "    for feature, score in features.items():\n",
    "        if abs(score) > threshold:\n",
    "            graph.edge(feature, factor, label=f\"{score:.2f}\", fontsize='14')\n",
    "\n",
    "# Save the graph as a PDF or PNG\n",
    "output_directory = os.path.join(current_directory, '04-summary')\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "graph_output_path = os.path.join(output_directory, 'sem_model_graph_top40')\n",
    "graph.render(graph_output_path)\n",
    "\n",
    "print(f\"Model graph saved to: {graph_output_path}.pdf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
