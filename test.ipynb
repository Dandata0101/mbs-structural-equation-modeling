{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyreadstat\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Paths to input files and output directories\n",
    "current_directory = os.getcwd()\n",
    "sav_file_path = os.path.join(current_directory, '01-data', 'yaleoutput.sav')\n",
    "rename_excel_path = os.path.join(current_directory, '01-data', 'column_rename_mapping.xlsx')\n",
    "output_excel_path = os.path.join(current_directory, '03-output', 'dfyale.xlsx')\n",
    "\n",
    "# Read the .sav file\n",
    "df, meta = pyreadstat.read_sav(sav_file_path)\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['starttime', 'endtime']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Load the rename mapping from the Excel file\n",
    "rename_df = pd.read_excel(rename_excel_path, sheet_name=\"Mapping\")\n",
    "column_rename_mapping = dict(zip(rename_df['OldName'], rename_df['NewName']))\n",
    "\n",
    "# Rename the columns\n",
    "df.rename(columns=column_rename_mapping, inplace=True)\n",
    "\n",
    "# Drop columns ending in '_select'\n",
    "select_columns = [col for col in df.columns if col.endswith('_select')]\n",
    "df.drop(columns=select_columns, inplace=True)\n",
    "\n",
    "# Get the current year\n",
    "current_year = datetime.now().year\n",
    "\n",
    "# Compute ages based on the current year\n",
    "df['age'] = current_year - df['birthyr']\n",
    "\n",
    "# Define age group bins with \"65+\" as the last group\n",
    "bins = [0, 17, 24, 34, 44, 54, 64, 100]\n",
    "labels = ['0-17', '18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "\n",
    "# Categorize ages into the specified groups\n",
    "df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Convert all numerical columns to floats and round to one decimal place\n",
    "for col in df.select_dtypes(include=['number']).columns:\n",
    "    df[col] = df[col].astype(float).round(1)\n",
    "\n",
    "# Load the grouping data from the Excel file\n",
    "grouping_df = pd.read_excel(rename_excel_path, sheet_name=\"Grouping\")\n",
    "\n",
    "# Mapping functions by suffix\n",
    "suffix_mappings = {\n",
    "    '_impact': dict(zip(grouping_df[grouping_df['Suffix'] == '_impact']['Code'], grouping_df[grouping_df['Suffix'] == '_impact']['Recode'])),\n",
    "    '_confidence': dict(zip(grouping_df[grouping_df['Suffix'] == '_confidence']['Code'], grouping_df[grouping_df['Suffix'] == '_confidence']['Recode'])),\n",
    "    '_important': dict(zip(grouping_df[grouping_df['Suffix'] == '_important']['Code'], grouping_df[grouping_df['Suffix'] == '_important']['Recode'])),\n",
    "    '_likelihood': dict(zip(grouping_df[grouping_df['Suffix'] == '_likelihood']['Code'], grouping_df[grouping_df['Suffix'] == '_likelihood']['Recode'])),\n",
    "    '_agreement': dict(zip(grouping_df[grouping_df['Suffix'] == '_agreement']['Code'], grouping_df[grouping_df['Suffix'] == '_agreement']['Recode'])),\n",
    "    'gender': dict(zip(grouping_df[grouping_df['Suffix'] == 'gender']['Code'], grouping_df[grouping_df['Suffix'] == 'gender']['Recode'])),\n",
    "    'hhi': dict(zip(grouping_df[grouping_df['Suffix'] == 'hhi']['Code'], grouping_df[grouping_df['Suffix'] == 'hhi']['Recode'])),\n",
    "    'marstat': dict(zip(grouping_df[grouping_df['Suffix'] == 'marstat']['Code'], grouping_df[grouping_df['Suffix'] == 'marstat']['Recode'])),\n",
    "    'educ': dict(zip(grouping_df[grouping_df['Suffix'] == 'educ']['Code'], grouping_df[grouping_df['Suffix'] == 'educ']['Recode'])),\n",
    "    'race': dict(zip(grouping_df[grouping_df['Suffix'] == 'race']['Code'], grouping_df[grouping_df['Suffix'] == 'race']['Recode'])),\n",
    "    'state': dict(zip(grouping_df[grouping_df['Suffix'] == 'state']['Code'], grouping_df[grouping_df['Suffix'] == 'state']['Recode'])),\n",
    "    'employ': dict(zip(grouping_df[grouping_df['Suffix'] == 'employ']['Code'], grouping_df[grouping_df['Suffix'] == 'employ']['Recode'])),\n",
    "    'Xvar_politics': dict(zip(grouping_df[grouping_df['Suffix'] == 'Xvar_politics']['Code'], grouping_df[grouping_df['Suffix'] == 'Xvar_politics']['Recode'])),\n",
    "    'Xvar_Q04_R01_ed_courses_CS': dict(zip(grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R01_ed_courses_CS']['Code'], grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R01_ed_courses_CS']['Recode'])),\n",
    "    'Xvar_Q04_R02_ed_undergrad_CS': dict(zip(grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R02_ed_undergrad_CS']['Code'], grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R02_ed_undergrad_CS']['Recode'])),\n",
    "    'Xvar_Q04_R03_ed_Grad_CS': dict(zip(grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R03_ed_Grad_CS']['Code'], grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R03_ed_Grad_CS']['Recode'])),\n",
    "    'Xvar_Q04_R04_ed_program_exp': dict(zip(grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R04_ed_program_exp']['Code'], grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R04_ed_program_exp']['Recode'])),\n",
    "    'Xvar_Q04_R05_ed_none': dict(zip(grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R05_ed_none']['Code'], grouping_df[grouping_df['Suffix'] == 'Xvar_Q04_R05_ed_none']['Recode'])),\n",
    "    'Y_Q05_SupportAI': dict(zip(grouping_df[grouping_df['Suffix'] == 'Y_Q05_SupportAI']['Code'], grouping_df[grouping_df['Suffix'] == 'Y_Q05_SupportAI']['Recode'])),\n",
    "    'Y_Q17_SupportDevHighlevelAI': dict(zip(grouping_df[grouping_df['Suffix'] == 'Y_Q17_SupportDevHighlevelAI']['Code'], grouping_df[grouping_df['Suffix'] == 'Y_Q17_SupportDevHighlevelAI']['Recode']))\n",
    "}\n",
    "\n",
    "# Function to recode columns based on suffix\n",
    "def apply_mapping(df, mapping_dict, suffix):\n",
    "    for col in df.columns:\n",
    "        if col.endswith(suffix):\n",
    "            df[col] = df[col].replace(mapping_dict)\n",
    "\n",
    "# Apply mappings for each suffix\n",
    "for suffix, mapping_dict in suffix_mappings.items():\n",
    "    apply_mapping(df, mapping_dict, suffix)\n",
    "\n",
    "# List of columns to be one-hot encoded\n",
    "columns_to_encode = ['Xvar_politics', 'Xvar_Q04_R01_ed_courses_CS', 'Xvar_Q04_R02_ed_undergrad_CS', 'Xvar_Q04_R03_ed_Grad_CS', 'Xvar_Q04_R04_ed_program_exp', 'Xvar_Q04_R05_ed_none']\n",
    "\n",
    "# Apply one-hot encoding to categorical columns, which will use 0 and 1\n",
    "df_dummies = pd.get_dummies(df, columns=columns_to_encode, drop_first=False)\n",
    "\n",
    "# Convert all Boolean columns to 0/1\n",
    "bool_columns = df_dummies.select_dtypes(include=['bool']).columns\n",
    "df_dummies[bool_columns] = df_dummies[bool_columns].astype(int)\n",
    "\n",
    "# Drop unwanted columns\n",
    "columns_to_drop = ['birthyr', 'pid7', 'votereg', 'ideo5', 'newsint', 'religpew', 'pew_churatd', 'pew_bornagain', 'pew_religimp', 'pew_prayer', 'Q03new_treat', 'q05b_treat', 'q12a_treat', 'q12_treat', 'q15_treat', 'Xvar_Q05b','Xvar_Q04_R05_ed_none_No','Xvar_Q04_R04_ed_program_exp_No','Xvar_Q04_R03_ed_Grad_CS_No','Xvar_Q04_R02_ed_undergrad_CS_No','Xvar_Q04_R01_ed_courses_CS_No']\n",
    "df_dummies = df_dummies.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Add the 'NA' category to all categorical columns and then fill missing values\n",
    "for col in df_dummies.select_dtypes(include='category').columns:\n",
    "    df_dummies[col] = df_dummies[col].cat.add_categories(['NA'])\n",
    "df_dummies = df_dummies.fillna('NA')\n",
    "\n",
    "weight_column = 'weight'\n",
    "\n",
    "# Identify columns starting with \"Xvar\" or \"Y_\"\n",
    "columns_to_weight = [col for col in df_dummies.columns if col.startswith('Xvar') or col.startswith('Y_')]\n",
    "\n",
    "# Ensure the columns are converted to numeric or boolean types before applying weights\n",
    "for col in columns_to_weight:\n",
    "    # Check if the column is categorical using isinstance with pd.CategoricalDtype\n",
    "    if isinstance(df_dummies[col].dtype, pd.CategoricalDtype):\n",
    "        df_dummies[col] = df_dummies[col].cat.codes\n",
    "    # Convert boolean columns to integers (0/1)\n",
    "    elif pd.api.types.is_bool_dtype(df_dummies[col]):\n",
    "        df_dummies[col] = df_dummies[col].astype(int)\n",
    "    # Convert remaining columns to numeric, coercing errors to NaN\n",
    "    df_dummies[col] = pd.to_numeric(df_dummies[col], errors='coerce')\n",
    "\n",
    "# Now apply the weight from the weight column to the identified columns\n",
    "for col in columns_to_weight:\n",
    "    df_dummies[col] = df_dummies[col] * df_dummies[weight_column]\n",
    "\n",
    "# Save the modified DataFrame to an Excel file\n",
    "df_dummies.to_excel(output_excel_path, index=False)\n",
    "\n",
    "print(f\"DataFrame with renamed columns is saved to {output_excel_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model graph saved to: /Users/danramirez/mbs-structural-equation-modeling/04-summary/sem_model_graph_grouped.pdf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from semopy import Model\n",
    "from graphviz import Digraph\n",
    "import os\n",
    "\n",
    "# Load the original data\n",
    "current_directory = os.getcwd()\n",
    "survey = os.path.join(current_directory, '03-output', 'dfyale.xlsx')\n",
    "sheet_name = 'Sheet1'\n",
    "df = pd.read_excel(survey, sheet_name=sheet_name)\n",
    "\n",
    "# List of specific endings to group by\n",
    "endings = [\n",
    "    \"Govt_War_challenges_important\", \"AI_Challenges_likelihood\",\n",
    "    \"AI_employee_Challenge_likelihood\", \"AI_ethics_important\",\n",
    "    \"AI_innovation_important\", \"AI_innovation_likelihood\",\n",
    "    \"AI_Law_likelihood\", \"AI_risk_impact\", \"AIHarm_likelihood\",\n",
    "    \"General_harm_likelihood\", \"General_risk_impact\",\n",
    "    \"Govt_War_challenges\", \"Govt_War_likelihood\",\n",
    "    \"Mgmt_Govt_confidence\", \"Mgmt_NGO_confidence\", \"Mgmt_Tech_confidence\",\n",
    "    \"Natural_Disaster_likelihood\", \"Natural_risk_impact\",\n",
    "    \"SocialEco_likelihood\", \"PI_Govt_confidence\",\n",
    "    \"PI_NGO_confidence\", \"PI_Tech_confidence\", \"SocialEco_risk_impact\"\n",
    "]\n",
    "\n",
    "# Dictionary to store columns for each group\n",
    "grouped_columns = {ending: [] for ending in endings}\n",
    "\n",
    "# Assign columns to each group based on their endings\n",
    "for col in df.columns:\n",
    "    for ending in endings:\n",
    "        if col.endswith(ending):\n",
    "            grouped_columns[ending].append(col)\n",
    "\n",
    "# Sum values in each group to create new columns\n",
    "for ending, cols in grouped_columns.items():\n",
    "    if cols:\n",
    "        df[f'Grouped_{ending}'] = df[cols].sum(axis=1)\n",
    "\n",
    "# Combine with dependent variables\n",
    "dependent_columns = ['Y_Q05_SupportAI']\n",
    "grouped_vars = [f'Grouped_{ending}' for ending in endings if grouped_columns[ending]]\n",
    "final_df = pd.concat([df[grouped_vars + dependent_columns]], axis=1)\n",
    "\n",
    "# SEM model with grouped features\n",
    "model_desc = f\"\"\"\n",
    "# Structural Model\n",
    "Y_Q05_SupportAI ~ {' + '.join(grouped_vars)}\n",
    "\"\"\"\n",
    "\n",
    "# Create the SEM model and load the dataset\n",
    "model = Model(model_desc)\n",
    "model.load_dataset(final_df)\n",
    "\n",
    "# Optimize the model\n",
    "model.fit()\n",
    "\n",
    "# Create a graph using the fdp engine in graphviz\n",
    "graph = Digraph(comment='SEM Model', engine='fdp', format='pdf')\n",
    "graph.attr(overlap='false', splines='true', dpi='300')\n",
    "\n",
    "# Add nodes (features) and edges (relationships) manually\n",
    "nodes = grouped_vars + ['Y_Q05_SupportAI']\n",
    "for node in nodes:\n",
    "    graph.node(node, node, shape='box', fontsize='20')\n",
    "\n",
    "# Add relationships (edges) between grouped features and the dependent variables\n",
    "for source in grouped_vars:\n",
    "    graph.edge(source, 'Y_Q05_SupportAI', fontsize='16')\n",
    "\n",
    "# Save the graph as a PDF\n",
    "output_directory = os.path.join(current_directory, '04-summary')\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "graph_output_path = os.path.join(output_directory, 'sem_model_graph_grouped')\n",
    "graph.render(graph_output_path)\n",
    "\n",
    "print(f\"Model graph saved to: {graph_output_path}.pdf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
