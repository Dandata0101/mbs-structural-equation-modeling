{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Filtered dataset to exclude ['Boomer'] in Generation\n",
      "\n",
      "Model created and dataset loaded into the model successfully for Entire Dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Fisher Information Matrix is not PD.Moore-Penrose inverse will be used instead of Cholesky decomposition. See 10.1109/TSP.2012.2208105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model optimization completed successfully for Entire Dataset.\n",
      "\n",
      "Full Results DataFrame for Entire Dataset:\n",
      "                                        lval  op  \\\n",
      "0              VAR11_PRIVACY_AI_Protect_Data   ~   \n",
      "1          VAR16_ETHICS_AI_Developed_Ethical   ~   \n",
      "2                                      Trust   ~   \n",
      "3                                      Trust   ~   \n",
      "4                                      Trust   ~   \n",
      "5                   VAR02_CG_AI_Training_Opo   ~   \n",
      "6   VAR12_PRIVACY_AI_Give_Consent_Data_Usage   ~   \n",
      "7          VAR25_FAIRNESS_AI_Treats_All_Fair   ~   \n",
      "8          VAR26_FAIRNESS_Should_Reduce_Bias   ~   \n",
      "9             VAR05_CG_AI_Training_Supported   ~   \n",
      "10               VAR03_CG_AI_Training_Access   ~   \n",
      "11                          Yvar_USE_AI_Work   ~   \n",
      "12                          Yvar_USE_AI_Work   ~   \n",
      "13                          Yvar_USE_AI_Work   ~   \n",
      "14                          Yvar_USE_AI_Work   ~   \n",
      "15                                     Trust  ~~   \n",
      "16                                     Trust  ~~   \n",
      "17                                     Trust  ~~   \n",
      "18                               Ease_of_Use  ~~   \n",
      "19                               Ease_of_Use  ~~   \n",
      "20                                  Fairness  ~~   \n",
      "21                                  Training  ~~   \n",
      "22                                  Training  ~~   \n",
      "23                                  Training  ~~   \n",
      "24             VAR11_PRIVACY_AI_Protect_Data  ~~   \n",
      "25         VAR16_ETHICS_AI_Developed_Ethical  ~~   \n",
      "26                  VAR02_CG_AI_Training_Opo  ~~   \n",
      "27               VAR03_CG_AI_Training_Access  ~~   \n",
      "28            VAR05_CG_AI_Training_Supported  ~~   \n",
      "29  VAR12_PRIVACY_AI_Give_Consent_Data_Usage  ~~   \n",
      "30         VAR25_FAIRNESS_AI_Treats_All_Fair  ~~   \n",
      "31         VAR26_FAIRNESS_Should_Reduce_Bias  ~~   \n",
      "32                          Yvar_USE_AI_Work  ~~   \n",
      "\n",
      "                                        rval  Estimate  Std. Err    z-value  \\\n",
      "0                                      Trust  1.000000       NaN        NaN   \n",
      "1                                      Trust  0.000000  0.002781   0.000000   \n",
      "2              VAR11_PRIVACY_AI_Protect_Data  0.000000  0.014003   0.000000   \n",
      "3          VAR16_ETHICS_AI_Developed_Ethical  0.000000  0.036172   0.000000   \n",
      "4                                   Training  0.000000  0.494345   0.000000   \n",
      "5                                Ease_of_Use  1.000000       NaN        NaN   \n",
      "6                                Ease_of_Use  0.249879  0.890170   0.280709   \n",
      "7                                   Fairness  1.000000       NaN        NaN   \n",
      "8                                   Fairness  0.239309  2.522544   0.094868   \n",
      "9                                   Training  1.000000       NaN        NaN   \n",
      "10                                  Training  0.278774  0.902936   0.308742   \n",
      "11                                     Trust  0.218517  0.064728   3.375914   \n",
      "12                               Ease_of_Use  0.655940  2.524494   0.259830   \n",
      "13                                  Fairness  0.210934  2.216897   0.095148   \n",
      "14                                  Training  0.521027  1.767009   0.294864   \n",
      "15                               Ease_of_Use  0.000000  0.021414   0.000000   \n",
      "16                                  Fairness  0.000000  0.026332   0.000000   \n",
      "17                                     Trust  0.050000  0.135880   0.367970   \n",
      "18                                  Fairness  0.000000  0.021993   0.000000   \n",
      "19                               Ease_of_Use  0.050000  0.194721   0.256777   \n",
      "20                                  Fairness  0.050000  0.535701   0.093336   \n",
      "21                                  Training  0.050000  0.175727   0.284533   \n",
      "22                               Ease_of_Use  0.000000  0.020644   0.000000   \n",
      "23                                  Fairness  0.000000  0.025385   0.000000   \n",
      "24             VAR11_PRIVACY_AI_Protect_Data  0.689109  0.147827   4.661583   \n",
      "25         VAR16_ETHICS_AI_Developed_Ethical  0.650412  0.031908  20.383817   \n",
      "26                  VAR02_CG_AI_Training_Opo  0.484980  0.195856   2.476201   \n",
      "27               VAR03_CG_AI_Training_Access  0.436468  0.025376  17.200378   \n",
      "28            VAR05_CG_AI_Training_Supported  0.718364  0.178726   4.019362   \n",
      "29  VAR12_PRIVACY_AI_Give_Consent_Data_Usage  0.727171  0.037683  19.297280   \n",
      "30         VAR25_FAIRNESS_AI_Treats_All_Fair  0.785648  0.536904   1.463293   \n",
      "31         VAR26_FAIRNESS_Should_Reduce_Bias  0.585317  0.042013  13.931810   \n",
      "32                          Yvar_USE_AI_Work  0.896697  0.109509   8.188321   \n",
      "\n",
      "         p-value  \n",
      "0            NaN  \n",
      "1   1.000000e+00  \n",
      "2   1.000000e+00  \n",
      "3   1.000000e+00  \n",
      "4   1.000000e+00  \n",
      "5            NaN  \n",
      "6   7.789334e-01  \n",
      "7            NaN  \n",
      "8   9.244197e-01  \n",
      "9            NaN  \n",
      "10  7.575180e-01  \n",
      "11  7.357083e-04  \n",
      "12  7.949947e-01  \n",
      "13  9.241972e-01  \n",
      "14  7.680978e-01  \n",
      "15  1.000000e+00  \n",
      "16  1.000000e+00  \n",
      "17  7.128952e-01  \n",
      "18  1.000000e+00  \n",
      "19  7.973507e-01  \n",
      "20  9.256369e-01  \n",
      "21  7.760020e-01  \n",
      "22  1.000000e+00  \n",
      "23  1.000000e+00  \n",
      "24  3.137866e-06  \n",
      "25  0.000000e+00  \n",
      "26  1.327889e-02  \n",
      "27  0.000000e+00  \n",
      "28  5.835602e-05  \n",
      "29  0.000000e+00  \n",
      "30  1.433872e-01  \n",
      "31  0.000000e+00  \n",
      "32  2.220446e-16  \n",
      "\n",
      "P-values extracted successfully for Entire Dataset:\n",
      "         p-value\n",
      "0            NaN\n",
      "1   1.000000e+00\n",
      "2   1.000000e+00\n",
      "3   1.000000e+00\n",
      "4   1.000000e+00\n",
      "5            NaN\n",
      "6   7.789334e-01\n",
      "7            NaN\n",
      "8   9.244197e-01\n",
      "9            NaN\n",
      "10  7.575180e-01\n",
      "11  7.357083e-04\n",
      "12  7.949947e-01\n",
      "13  9.241972e-01\n",
      "14  7.680978e-01\n",
      "15  1.000000e+00\n",
      "16  1.000000e+00\n",
      "17  7.128952e-01\n",
      "18  1.000000e+00\n",
      "19  7.973507e-01\n",
      "20  9.256369e-01\n",
      "21  7.760020e-01\n",
      "22  1.000000e+00\n",
      "23  1.000000e+00\n",
      "24  3.137866e-06\n",
      "25  0.000000e+00\n",
      "26  1.327889e-02\n",
      "27  0.000000e+00\n",
      "28  5.835602e-05\n",
      "29  0.000000e+00\n",
      "30  1.433872e-01\n",
      "31  0.000000e+00\n",
      "32  2.220446e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ft/dwmcjbnd0b35z5n9yksp4jfh0000gn/T/ipykernel_19174/446269052.py:100: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  results = results.applymap(lambda x: np.nan if x in [\"Not estimated\", \"-\", None] else x)\n",
      "WARNING:root:Fisher Information Matrix is not PD.Moore-Penrose inverse will be used instead of Cholesky decomposition. See 10.1109/TSP.2012.2208105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEM results and hypothesis results saved to /Users/danramirez/mbs-structural-equation-modeling/04-summary/SEM_Results.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from semopy import Model, Optimizer\n",
    "\n",
    "# Set to 'Y' to segment results by a specific column or 'N' to run the model on the entire dataset\n",
    "segment_results = 'N'  # 'Y' or 'N'\n",
    "segmentation_column = 'Generation'  # The column used for segmentation if segment_results = 'Y'\n",
    "filter_column = 'Generation'  # The column used for filtering\n",
    "filter_values = ['Boomer']  # List of values to exclude from the dataset\n",
    "\n",
    "# Define the path to your dataset\n",
    "current_directory = os.getcwd()\n",
    "excel_path = os.path.join(current_directory, '01-data', 'TAM_DEF.xlsx')\n",
    "summary_dir = os.path.join(current_directory, '04-summary')\n",
    "\n",
    "# Ensure the summary directory exists\n",
    "if not os.path.exists(summary_dir):\n",
    "    os.makedirs(summary_dir)\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found. Please check the file path: {excel_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error during dataset loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Apply filtering if filter_column and filter_values are set\n",
    "if filter_column and filter_values:\n",
    "    df = df[~df[filter_column].isin(filter_values)]\n",
    "    print(f\"Filtered dataset to exclude {filter_values} in {filter_column}\")\n",
    "\n",
    "if segment_results == 'Y':\n",
    "    # Get unique values in the segmentation column\n",
    "    segments = df[segmentation_column].unique()\n",
    "    print(f\"Found {segmentation_column} segments: {segments}\")\n",
    "else:\n",
    "    # If not segmenting, treat the entire dataset as a single segment\n",
    "    segments = ['Entire Dataset']\n",
    "    df['Entire Dataset'] = 'Entire Dataset'  # Add a dummy column to facilitate the loop\n",
    "\n",
    "# Initialize a list to store hypothesis results for all segments\n",
    "all_hypothesis_results = []\n",
    "\n",
    "# Loop through each segment and run the SEM model\n",
    "for segment in segments:\n",
    "    df_segment = df[df[segmentation_column] == segment] if segment_results == 'Y' else df\n",
    "    \n",
    "    # Define the SEM model with the Yvar_Work_Personal and revised hypotheses\n",
    "    model_desc = \"\"\"\n",
    "    # Latent variables\n",
    "    Trust =~ VAR11_PRIVACY_AI_Protect_Data + VAR16_ETHICS_AI_Developed_Ethical\n",
    "    Ease_of_Use =~ VAR02_CG_AI_Training_Opo + VAR12_PRIVACY_AI_Give_Consent_Data_Usage\n",
    "    Fairness =~ VAR25_FAIRNESS_AI_Treats_All_Fair + VAR26_FAIRNESS_Should_Reduce_Bias\n",
    "    Training =~ VAR05_CG_AI_Training_Supported + VAR03_CG_AI_Training_Access\n",
    "\n",
    "    # Direct relationships with Yvar_Work_Personal\n",
    "    Yvar_Work_Personal ~ Trust\n",
    "    Yvar_Work_Personal ~ Ease_of_Use\n",
    "    Yvar_Work_Personal ~ Fairness\n",
    "    Yvar_Work_Personal ~ Training\n",
    "\n",
    "    # Relationships with latent variables\n",
    "    Trust ~ VAR11_PRIVACY_AI_Protect_Data\n",
    "    Trust ~ VAR16_ETHICS_AI_Developed_Ethical\n",
    "    Trust ~ Training\n",
    "\n",
    "    # Covariances (as needed)\n",
    "    Trust ~~ Ease_of_Use\n",
    "    Trust ~~ Fairness\n",
    "    Ease_of_Use ~~ Fairness\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the model and load the dataset into the model\n",
    "    try:\n",
    "        model = Model(model_desc)\n",
    "        model.load_dataset(df_segment)\n",
    "        print(f\"\\nModel created and dataset loaded into the model successfully for {segment}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model creation or dataset loading for {segment}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Optimize the model\n",
    "    try:\n",
    "        optim = Optimizer(model)\n",
    "        optim.optimize()\n",
    "        print(f\"Model optimization completed successfully for {segment}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model optimization for {segment}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Extract the results\n",
    "    try:\n",
    "        results = model.inspect()\n",
    "        # Convert any \"Not estimated\" or non-numeric values to NaN\n",
    "        results = results.applymap(lambda x: np.nan if x in [\"Not estimated\", \"-\", None] else x)\n",
    "        print(f\"\\nFull Results DataFrame for {segment}:\")\n",
    "        print(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during results extraction for {segment}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Attempt to extract p-values for the paths\n",
    "    try:\n",
    "        pvalues = results[['p-value']].apply(pd.to_numeric, errors='coerce')  # Convert to numeric, set errors to NaN\n",
    "        print(f\"\\nP-values extracted successfully for {segment}:\")\n",
    "        print(pvalues)\n",
    "    except KeyError:\n",
    "        print(f\"\\nUnable to extract p-values for {segment}. Check the results DataFrame above for available data.\")\n",
    "        continue\n",
    "\n",
    "    # Define hypotheses and their corresponding paths based on revised hypotheses\n",
    "    hypothesis_criteria = [\n",
    "        (\"Hypothesis 1: Trust influences Usage\", 'Yvar_Work_Personal ~ Trust'),\n",
    "        (\"Hypothesis 2: Ease of Use influences Usage\", 'Yvar_Work_Personal ~ Ease_of_Use'),\n",
    "        (\"Hypothesis 3: Fairness influences Usage\", 'Yvar_Work_Personal ~ Fairness'),\n",
    "        (\"Hypothesis 4: AI-related training and career growth opportunities influence Usage\", 'Yvar_Work_Personal ~ Training'),\n",
    "        (\"Hypothesis 5: Privacy and Data Protection influence Trust\", 'Trust ~ VAR11_PRIVACY_AI_Protect_Data'),\n",
    "        (\"Hypothesis 6: Ethical Considerations influence Trust\", 'Trust ~ VAR16_ETHICS_AI_Developed_Ethical')\n",
    "    ]\n",
    "\n",
    "    # Create a DataFrame to store the hypothesis results for the current segment\n",
    "    hypothesis_results = []\n",
    "\n",
    "    # Determine whether each hypothesis is accepted or rejected\n",
    "    for hyp, path in hypothesis_criteria:\n",
    "        matching_paths = results[(results['lval'] == path.split(' ~ ')[0]) & \n",
    "                                 (results['rval'] == path.split(' ~ ')[1])]\n",
    "        if not matching_paths.empty:\n",
    "            p_value = matching_paths['p-value'].values[0]\n",
    "            estimate = matching_paths['Estimate'].values[0]\n",
    "            std_err = matching_paths['Std. Err'].values[0]\n",
    "            z_value = matching_paths['z-value'].values[0]\n",
    "            result = 'Accepted' if not np.isnan(p_value) and p_value < 0.05 else 'Rejected'\n",
    "            hypothesis_results.append({\n",
    "                'Segment': segment,\n",
    "                'Hypothesis': hyp,\n",
    "                'p-value': p_value,\n",
    "                'Estimate': estimate,\n",
    "                'Std. Err': std_err,\n",
    "                'z-value': z_value,\n",
    "                'Result': result\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Path {path} not found in results for {segment}. Please check the available paths.\")\n",
    "            hypothesis_results.append({\n",
    "                'Segment': segment,\n",
    "                'Hypothesis': hyp,\n",
    "                'p-value': np.nan,\n",
    "                'Estimate': np.nan,\n",
    "                'Std. Err': np.nan,\n",
    "                'z-value': np.nan,\n",
    "                'Result': 'Path Not Found'\n",
    "            })\n",
    "\n",
    "    # Append the current segment's hypothesis results to the overall list\n",
    "    all_hypothesis_results.extend(hypothesis_results)\n",
    "\n",
    "# Convert the overall hypothesis results to a DataFrame\n",
    "all_hypothesis_df = pd.DataFrame(all_hypothesis_results)\n",
    "\n",
    "# Save the results to Excel\n",
    "output_path = os.path.join(summary_dir, 'SEM_Results.xlsx')\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    # Write the overall hypothesis results\n",
    "    all_hypothesis_df.to_excel(writer, sheet_name='Hypothesis Results', index=False)\n",
    "    \n",
    "    # Loop through each segment and write the SEM results for each segment in a separate tab\n",
    "    for segment in segments:\n",
    "        df_segment_results = df[df[segmentation_column] == segment] if segment_results == 'Y' else df\n",
    "        # Extract the SEM results for the segment\n",
    "        model = Model(model_desc)\n",
    "        model.load_dataset(df_segment_results)\n",
    "        optim = Optimizer(model)\n",
    "        optim.optimize()\n",
    "        results = model.inspect()\n",
    "        # Write SEM results to its own sheet\n",
    "        results.to_excel(writer, sheet_name=f'SEM Results - {segment}')\n",
    "\n",
    "print(f\"SEM results and hypothesis results saved to {output_path}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
