{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from semopy import Model, Optimizer\n",
    "\n",
    "# Parameters\n",
    "p_value_threshold = 0.05  # Set the threshold for p-value\n",
    "dependent_variable = 'Yvar_Work_Personal'  # Set the dependent variable\n",
    "segment_results = 'Y'  # 'Y' or 'N'\n",
    "segmentation_column = 'Generation'  # The column used for segmentation if segment_results = 'Y'\n",
    "filter_column = 'Generation'  # The column used for filtering\n",
    "filter_values = ['Boomer']  # List of values to exclude from the dataset\n",
    "\n",
    "# Define the path to your dataset\n",
    "current_directory = os.getcwd()\n",
    "excel_path = os.path.join(current_directory, '01-data', 'TAM_DEF.xlsx')\n",
    "summary_dir = os.path.join(current_directory, '04-summary')\n",
    "\n",
    "# Ensure the summary directory exists\n",
    "if not os.path.exists(summary_dir):\n",
    "    os.makedirs(summary_dir)\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found. Please check the file path: {excel_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error during dataset loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Apply filtering if filter_column and filter_values are set\n",
    "if filter_column and filter_values:\n",
    "    df = df[~df[filter_column].isin(filter_values)]\n",
    "    print(f\"Filtered dataset to exclude {filter_values} in {filter_column}\")\n",
    "\n",
    "if segment_results == 'Y':\n",
    "    # Get unique values in the segmentation column\n",
    "    segments = df[segmentation_column].unique()\n",
    "    print(f\"Found {segmentation_column} segments: {segments}\")\n",
    "else:\n",
    "    # If not segmenting, treat the entire dataset as a single segment\n",
    "    segments = ['Entire Dataset']\n",
    "    df['Entire Dataset'] = 'Entire Dataset'  # Add a dummy column to facilitate the loop\n",
    "\n",
    "# Initialize a list to store hypothesis results for all segments\n",
    "all_hypothesis_results = []\n",
    "\n",
    "# Loop through each segment and run the SEM model\n",
    "for segment in segments:\n",
    "    df_segment = df[df[segmentation_column] == segment] if segment_results == 'Y' else df\n",
    "    \n",
    "    # Define the SEM model without the 'Personalization' latent variable\n",
    "    model_desc = f\"\"\"\n",
    "    # Latent variables\n",
    "    Trust =~ VAR11_PRIVACY_AI_Protect_Data + VAR16_ETHICS_AI_Developed_Ethical + VAR25_FAIRNESS_AI_Treats_All_Fair + VAR26_FAIRNESS_Should_Reduce_Bias\n",
    "    Ease_of_Use =~ VAR02_CG_AI_Training_Opo + VAR12_PRIVACY_AI_Give_Consent_Data_Usage\n",
    "    Training =~ VAR05_CG_AI_Training_Supported + VAR03_CG_AI_Training_Access\n",
    "\n",
    "    # Direct relationships with Usage ({dependent_variable})\n",
    "    {dependent_variable} ~ Trust\n",
    "    {dependent_variable} ~ Ease_of_Use\n",
    "    {dependent_variable} ~ Training\n",
    "\n",
    "    # Relationships with latent variables\n",
    "    Trust ~ Training\n",
    "    Trust ~ Ease_of_Use\n",
    "    Trust ~ VAR16_ETHICS_AI_Developed_Ethical + VAR11_PRIVACY_AI_Protect_Data + VAR25_FAIRNESS_AI_Treats_All_Fair + VAR26_FAIRNESS_Should_Reduce_Bias\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the model and load the dataset into the model\n",
    "    try:\n",
    "        model = Model(model_desc)\n",
    "        model.load_dataset(df_segment)\n",
    "        print(f\"\\nModel created and dataset loaded into the model successfully for {segment}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model creation or dataset loading for {segment}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Optimize the model\n",
    "    try:\n",
    "        optim = Optimizer(model)\n",
    "        optim.optimize()\n",
    "        print(f\"Model optimization completed successfully for {segment}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model optimization for {segment}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Extract the results\n",
    "    try:\n",
    "        results = model.inspect()\n",
    "        # Convert any \"Not estimated\" or non-numeric values to NaN\n",
    "        results = results.applymap(lambda x: np.nan if x in [\"Not estimated\", \"-\", None] else x)\n",
    "        print(f\"\\nFull Results DataFrame for {segment}:\")\n",
    "        print(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during results extraction for {segment}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Attempt to extract p-values for the paths\n",
    "    try:\n",
    "        pvalues = results[['p-value']].apply(pd.to_numeric, errors='coerce')  # Convert to numeric, set errors to NaN\n",
    "        print(f\"\\nP-values extracted successfully for {segment}:\")\n",
    "        print(pvalues)\n",
    "    except KeyError:\n",
    "        print(f\"\\nUnable to extract p-values for {segment}. Check the results DataFrame above for available data.\")\n",
    "        continue\n",
    "\n",
    "    # Define hypotheses and their corresponding paths based on the five hypotheses\n",
    "    hypothesis_criteria = [\n",
    "        (f\"Hypothesis 1: AI Training and Career Growth Opportunities Influence Ease of Use and Trust\", f'{dependent_variable} ~ Training'),\n",
    "        (f\"Hypothesis 2: Explainability and Fairness Influence User Trust and AI Adoption\", f'{dependent_variable} ~ Trust'),\n",
    "        (f\"Hypothesis 3: Ethical Development and Social Responsibility Influence User Trust\", 'Trust ~ VAR16_ETHICS_AI_Developed_Ethical'),\n",
    "        (f\"Hypothesis 4: AI-Driven Personalization Enhances User Experience\", f'{dependent_variable} ~ Ease_of_Use'),\n",
    "        (f\"Hypothesis 5: AI Training Accessibility and Accountability Influence User Trust and Usage\", 'Trust ~ Training')\n",
    "    ]\n",
    "\n",
    "    # Create a DataFrame to store the hypothesis results for the current segment\n",
    "    hypothesis_results = []\n",
    "\n",
    "    # Determine whether each hypothesis is accepted or rejected\n",
    "    for hyp, path in hypothesis_criteria:\n",
    "        matching_paths = results[(results['lval'] == path.split(' ~ ')[0]) & \n",
    "                                 (results['rval'] == path.split(' ~ ')[1])]\n",
    "        if not matching_paths.empty:\n",
    "            p_value = matching_paths['p-value'].values[0]\n",
    "            estimate = matching_paths['Estimate'].values[0]\n",
    "            std_err = matching_paths['Std. Err'].values[0]\n",
    "            z_value = matching_paths['z-value'].values[0]\n",
    "            result = 'Accepted' if not np.isnan(p_value) and p_value < p_value_threshold else 'Rejected'\n",
    "            hypothesis_results.append({\n",
    "                'Segment': segment,\n",
    "                'Hypothesis': hyp,\n",
    "                'p-value': p_value,\n",
    "                'Estimate': estimate,\n",
    "                'Std. Err': std_err,\n",
    "                'z-value': z_value,\n",
    "                'Result': result\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Path {path} not found in results for {segment}. Please check the available paths.\")\n",
    "            hypothesis_results.append({\n",
    "                'Segment': segment,\n",
    "                'Hypothesis': hyp,\n",
    "                'p-value': np.nan,\n",
    "                'Estimate': np.nan,\n",
    "                'Std. Err': np.nan,\n",
    "                'z-value': np.nan,\n",
    "                'Result': 'Path Not Found'\n",
    "            })\n",
    "\n",
    "    # Append the current segment's hypothesis results to the overall list\n",
    "    all_hypothesis_results.extend(hypothesis_results)\n",
    "\n",
    "# Convert the overall hypothesis results to a DataFrame\n",
    "all_hypothesis_df = pd.DataFrame(all_hypothesis_results)\n",
    "\n",
    "# Save the results to Excel\n",
    "output_path = os.path.join(summary_dir, 'SEM_Results.xlsx')\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    # Write the overall hypothesis results\n",
    "    all_hypothesis_df.to_excel(writer, sheet_name='Hypothesis Results', index=False)\n",
    "    \n",
    "    # Loop through each segment and write the SEM results for each segment in a separate tab\n",
    "    for segment in segments:\n",
    "        df_segment_results = df[df[segmentation_column] == segment] if segment_results == 'Y' else df\n",
    "        # Extract the SEM results for the segment\n",
    "        model = Model(model_desc)\n",
    "        model.load_dataset(df_segment_results)\n",
    "        optim = Optimizer(model)\n",
    "        optim.optimize()\n",
    "        results = model.inspect()\n",
    "        # Write SEM results to its own sheet\n",
    "        results.to_excel(writer, sheet_name=f'SEM Results - {segment}')\n",
    "\n",
    "print(f\"SEM results and hypothesis results saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Define the nodes and their positions for a left-to-right layout\n",
    "nodes = {\n",
    "    \"Perceived Security\": (0, 10),\n",
    "    \"Perceived Privacy\": (0, 9),\n",
    "    \"Transparency\": (0, 8),\n",
    "    \"Explainability\": (0, 7),\n",
    "    \"Perceived Fairness\": (0, 6),\n",
    "    \"Perceived Non-Discrimination\": (0, 5),\n",
    "    \"Training Opportunities\": (0, 4),\n",
    "    \"Career Growth Opportunities\": (0, 3),\n",
    "    \"Perceived Safety\": (0, 2),\n",
    "    \"Ethical Considerations\": (0, 1),\n",
    "    \"Perceived Positive Social Impact\": (0, 0),\n",
    "    \"Ease of Use\": (5, 5),\n",
    "    \"Trust in AI\": (10, 5),\n",
    "    \"Likelihood of AI Usage at Work\": (15, 7),\n",
    "    \"Likelihood of AI Usage in Daily Life\": (15, 3),\n",
    "    \"Likelihood of AI Adoption\": (20, 5)\n",
    "}\n",
    "\n",
    "# Define the edges based on the relationships\n",
    "edges = [\n",
    "    (\"Perceived Security\", \"Ease of Use\"),\n",
    "    (\"Perceived Privacy\", \"Ease of Use\"),\n",
    "    (\"Transparency\", \"Ease of Use\"),\n",
    "    (\"Explainability\", \"Ease of Use\"),\n",
    "    (\"Perceived Fairness\", \"Ease of Use\"),\n",
    "    (\"Perceived Non-Discrimination\", \"Ease of Use\"),\n",
    "    (\"Training Opportunities\", \"Ease of Use\"),\n",
    "    (\"Career Growth Opportunities\", \"Ease of Use\"),\n",
    "    (\"Perceived Safety\", \"Trust in AI\"),\n",
    "    (\"Ethical Considerations\", \"Trust in AI\"),\n",
    "    (\"Perceived Positive Social Impact\", \"Trust in AI\"),\n",
    "    (\"Training Opportunities\", \"Trust in AI\"),\n",
    "    (\"Career Growth Opportunities\", \"Trust in AI\"),\n",
    "    (\"Ease of Use\", \"Trust in AI\"),\n",
    "    (\"Ease of Use\", \"Likelihood of AI Usage at Work\"),\n",
    "    (\"Ease of Use\", \"Likelihood of AI Usage in Daily Life\"),\n",
    "    (\"Trust in AI\", \"Likelihood of AI Usage at Work\"),\n",
    "    (\"Trust in AI\", \"Likelihood of AI Usage in Daily Life\"),\n",
    "    (\"Trust in AI\", \"Likelihood of AI Adoption\"),\n",
    "    (\"Likelihood of AI Usage at Work\", \"Likelihood of AI Adoption\"),\n",
    "    (\"Likelihood of AI Usage in Daily Life\", \"Likelihood of AI Adoption\")\n",
    "]\n",
    "\n",
    "# Add nodes and edges to the graph\n",
    "G.add_nodes_from(nodes.keys())\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Define the positions of the nodes\n",
    "pos = {node: (x, y) for node, (x, y) in nodes.items()}\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(20, 8))\n",
    "nx.draw_networkx(G, pos, with_labels=True, node_size=3000, node_color=\"lightblue\", font_size=9, font_weight=\"bold\", arrows=True, arrowstyle=\"->\", arrowsize=15)\n",
    "plt.title(\"A conceptual diagram illustrating the relationship between AI-related factors and user trust/adoption\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Parameters\n",
    "segment_results = 'Y'  # Set to 'Y' to segment results by a specific column or 'N' to run the model on the entire dataset\n",
    "segmentation_column = 'Generation'  # The column used for segmentation\n",
    "filter_column = 'Generation'  # The column used for filtering\n",
    "filter_values = ['Boomer']  # List of values to exclude from the dataset\n",
    "pvalue_threshold = 0.05  # P-value threshold for significance\n",
    "y_column = 'Yvar_USE_AI_Work'  # Dependent variable\n",
    "x_column_prefix = 'VAR'  # Prefix for independent variables\n",
    "output_filename_template = 'SEM_Results_{segmentation_column}_Detailed.xlsx'  # Template for output filename\n",
    "\n",
    "# Define the path to your dataset\n",
    "current_directory = os.getcwd()\n",
    "excel_path = os.path.join(current_directory, '01-data', 'TAM_DEF.xlsx')\n",
    "summary_dir = os.path.join(current_directory, '04-summary')\n",
    "\n",
    "# Ensure the summary directory exists\n",
    "if not os.path.exists(summary_dir):\n",
    "    os.makedirs(summary_dir)\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found. Please check the file path: {excel_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error during dataset loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Apply filtering if filter_column and filter_values are set\n",
    "if filter_column and filter_values:\n",
    "    df = df[~df[filter_column].isin(filter_values)]\n",
    "    print(f\"Filtered dataset to exclude {filter_values} in {filter_column}\")\n",
    "\n",
    "if segment_results == 'Y':\n",
    "    # Get unique values in the segmentation column\n",
    "    segments = df[segmentation_column].unique()\n",
    "    print(f\"Found {segmentation_column} segments: {segments}\")\n",
    "else:\n",
    "    # If not segmenting, treat the entire dataset as a single segment\n",
    "    segments = ['Entire Dataset']\n",
    "    df['Entire Dataset'] = 'Entire Dataset'  # Add a dummy column to facilitate the loop\n",
    "\n",
    "# Loop through each segment and run the regression model\n",
    "all_results = []\n",
    "top_variables_set = set()\n",
    "\n",
    "for segment in segments:\n",
    "    df_segment = df[df[segmentation_column] == segment] if segment != 'Entire Dataset' else df\n",
    "    \n",
    "    # Define X (independent variables) and y (dependent variable)\n",
    "    X = df_segment.filter(regex=f'^{x_column_prefix}')\n",
    "    y = df_segment[y_column]\n",
    "    \n",
    "    # Add a constant (intercept) to the model\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Fit the OLS model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Extract the results, filtering by p-value threshold\n",
    "    significant_results = model.summary2().tables[1]\n",
    "    significant_results = significant_results[significant_results['P>|t|'] <= pvalue_threshold]\n",
    "    \n",
    "    # Exclude the constant from the top variables set\n",
    "    top_variables_set.update(var for var in significant_results.index.tolist() if var != 'const')\n",
    "    \n",
    "    # Append results to the list for later export\n",
    "    segment_results = significant_results.reset_index()\n",
    "    segment_results.insert(0, 'Segment', segment)\n",
    "    segment_results.insert(1, 'R2 Value', model.rsquared)\n",
    "    all_results.append(segment_results)\n",
    "\n",
    "# Combine the results from all segments into a DataFrame\n",
    "all_results_df = pd.concat(all_results)\n",
    "\n",
    "# Run overall regression with combined top variables\n",
    "combined_X = df[list(top_variables_set)]\n",
    "combined_X = sm.add_constant(combined_X)\n",
    "combined_model = sm.OLS(df[y_column], combined_X).fit()\n",
    "\n",
    "# Prepare overall model results for export\n",
    "overall_results = combined_model.summary2().tables[1].reset_index()\n",
    "overall_results.insert(0, 'Segment', 'Overall')\n",
    "overall_results.insert(1, 'R2 Value', combined_model.rsquared)\n",
    "\n",
    "# Save the results to Excel, including the segmentation column name in the filename\n",
    "output_filename = output_filename_template.format(segmentation_column=segmentation_column)\n",
    "output_path = os.path.join(summary_dir, output_filename)\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
    "    # Write the overall hypothesis results\n",
    "    all_results_df.to_excel(writer, sheet_name='Hypothesis Results', index=False)\n",
    "    \n",
    "    # Write the overall regression results\n",
    "    overall_results.to_excel(writer, sheet_name='Overall Regression', index=False)\n",
    "\n",
    "    # Apply formatting\n",
    "    workbook = writer.book\n",
    "    header_format = workbook.add_format({\n",
    "        'bold': True, 'text_wrap': True, 'align': 'center', 'valign': 'center', 'bg_color': '#D9EAD3'\n",
    "    })\n",
    "    for sheet_name in writer.sheets:\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "        worksheet.set_row(0, None, header_format)\n",
    "        worksheet.set_column('A:G', 20)  # Adjust column width for better readability\n",
    "\n",
    "    print(f\"Summary statistics and regression results saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Define the nodes and their positions for a left-to-right layout\n",
    "nodes = {\n",
    "    \"Perceived Security\": (0, 10),\n",
    "    \"Perceived Privacy\": (0, 9),\n",
    "    \"Transparency\": (0, 8),\n",
    "    \"Explainability\": (0, 7),\n",
    "    \"Perceived Fairness\": (0, 6),\n",
    "    \"Perceived Non-Discrimination\": (0, 5),\n",
    "    \"Training Opportunities\": (0, 4),\n",
    "    \"Career Growth Opportunities\": (0, 3),\n",
    "    \"Perceived Safety\": (0, 2),\n",
    "    \"Ethical Considerations\": (0, 1),\n",
    "    \"Perceived Positive Social Impact\": (0, 0),\n",
    "    \"Ease of Use\": (5, 5),\n",
    "    \"Trust in AI\": (10, 5),\n",
    "    \"Likelihood of AI Usage at Work\": (15, 7),\n",
    "    \"Likelihood of AI Usage in Daily Life\": (15, 3),\n",
    "    \"Likelihood of AI Adoption\": (20, 5)\n",
    "}\n",
    "\n",
    "# Define the edges based on the relationships\n",
    "edges = [\n",
    "    (\"Perceived Security\", \"Ease of Use\"),\n",
    "    (\"Perceived Privacy\", \"Ease of Use\"),\n",
    "    (\"Transparency\", \"Ease of Use\"),\n",
    "    (\"Explainability\", \"Ease of Use\"),\n",
    "    (\"Perceived Fairness\", \"Ease of Use\"),\n",
    "    (\"Perceived Non-Discrimination\", \"Ease of Use\"),\n",
    "    (\"Training Opportunities\", \"Ease of Use\"),\n",
    "    (\"Career Growth Opportunities\", \"Ease of Use\"),\n",
    "    (\"Perceived Safety\", \"Trust in AI\"),\n",
    "    (\"Ethical Considerations\", \"Trust in AI\"),\n",
    "    (\"Perceived Positive Social Impact\", \"Trust in AI\"),\n",
    "    (\"Training Opportunities\", \"Trust in AI\"),\n",
    "    (\"Career Growth Opportunities\", \"Trust in AI\"),\n",
    "    (\"Ease of Use\", \"Trust in AI\"),\n",
    "    (\"Ease of Use\", \"Likelihood of AI Usage at Work\"),\n",
    "    (\"Ease of Use\", \"Likelihood of AI Usage in Daily Life\"),\n",
    "    (\"Trust in AI\", \"Likelihood of AI Usage at Work\"),\n",
    "    (\"Trust in AI\", \"Likelihood of AI Usage in Daily Life\"),\n",
    "    (\"Trust in AI\", \"Likelihood of AI Adoption\"),\n",
    "    (\"Likelihood of AI Usage at Work\", \"Likelihood of AI Adoption\"),\n",
    "    (\"Likelihood of AI Usage in Daily Life\", \"Likelihood of AI Adoption\")\n",
    "]\n",
    "\n",
    "# Add nodes and edges to the graph\n",
    "G.add_nodes_from(nodes.keys())\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Define the positions of the nodes\n",
    "pos = {node: (x, y) for node, (x, y) in nodes.items()}\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(20, 8))\n",
    "nx.draw_networkx(G, pos, with_labels=True, node_size=3000, node_color=\"lightblue\", font_size=9, font_weight=\"bold\", arrows=True, arrowstyle=\"->\", arrowsize=15)\n",
    "plt.title(\"A conceptual diagram illustrating the relationship between AI-related factors and user trust/adoption\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
