{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exploratory Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Parameters\n",
    "segment_results = 'Y'  # Set to 'Y' to segment results by a specific column or 'N' to run the model on the entire dataset\n",
    "segmentation_column = 'Generation'  # The column used for segmentation\n",
    "filter_column = None  # The column used for filtering\n",
    "filter_values = None  # List of values to exclude from the dataset\n",
    "pvalue_threshold = 0.05  # P-value threshold for significance\n",
    "y_column = 'Yvar_USE_AI_Work'  # Dependent variable\n",
    "x_column_prefix = 'VAR'  # Prefix for independent variables\n",
    "output_filename_template = 'Regression_Results_{segmentation_column}_Detailed.xlsx'  # Template for output filename\n",
    "\n",
    "# Define the path to your dataset\n",
    "current_directory = os.getcwd()\n",
    "excel_path = os.path.join(current_directory, '01-data', 'TAM_DEF.xlsx')\n",
    "summary_dir = os.path.join(current_directory, '04-summary')\n",
    "\n",
    "# Ensure the summary directory exists\n",
    "if not os.path.exists(summary_dir):\n",
    "    os.makedirs(summary_dir)\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found. Please check the file path: {excel_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error during dataset loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Apply filtering if filter_column and filter_values are set\n",
    "if filter_column and filter_values:\n",
    "    df = df[~df[filter_column].isin(filter_values)]\n",
    "    print(f\"Filtered dataset to exclude {filter_values} in {filter_column}\")\n",
    "\n",
    "if segment_results == 'Y':\n",
    "    # Get unique values in the segmentation column\n",
    "    segments = df[segmentation_column].unique()\n",
    "    print(f\"Found {segmentation_column} segments: {segments}\")\n",
    "else:\n",
    "    # If not segmenting, treat the entire dataset as a single segment\n",
    "    segments = ['Entire Dataset']\n",
    "    df['Entire Dataset'] = 'Entire Dataset'  # Add a dummy column to facilitate the loop\n",
    "\n",
    "# Loop through each segment and run the regression model\n",
    "all_results = []\n",
    "top_variables_set = set()\n",
    "\n",
    "for segment in segments:\n",
    "    df_segment = df[df[segmentation_column] == segment] if segment != 'Entire Dataset' else df\n",
    "    \n",
    "    # Define X (independent variables) and y (dependent variable)\n",
    "    X = df_segment.filter(regex=f'^{x_column_prefix}')\n",
    "    y = df_segment[y_column]\n",
    "    \n",
    "    # Add a constant (intercept) to the model\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Fit the OLS model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Extract the results, filtering by p-value threshold\n",
    "    significant_results = model.summary2().tables[1]\n",
    "    significant_results = significant_results[significant_results['P>|t|'] <= pvalue_threshold]\n",
    "    \n",
    "    # Exclude the constant from the top variables set\n",
    "    top_variables_set.update(var for var in significant_results.index.tolist() if var != 'const')\n",
    "    \n",
    "    # Append results to the list for later export\n",
    "    segment_results = significant_results.reset_index()\n",
    "    segment_results.insert(0, 'Segment', segment)\n",
    "    segment_results.insert(1, 'R2 Value', model.rsquared)\n",
    "    all_results.append(segment_results)\n",
    "\n",
    "# Combine the results from all segments into a DataFrame\n",
    "all_results_df = pd.concat(all_results)\n",
    "\n",
    "# Run overall regression with combined top variables\n",
    "combined_X = df[list(top_variables_set)]\n",
    "combined_X = sm.add_constant(combined_X)\n",
    "combined_model = sm.OLS(df[y_column], combined_X).fit()\n",
    "\n",
    "# Prepare overall model results for export\n",
    "overall_results = combined_model.summary2().tables[1].reset_index()\n",
    "overall_results.insert(0, 'Segment', 'Overall')\n",
    "overall_results.insert(1, 'R2 Value', combined_model.rsquared)\n",
    "\n",
    "# Save the results to Excel, including the segmentation column name in the filename\n",
    "output_filename = output_filename_template.format(segmentation_column=segmentation_column)\n",
    "output_path = os.path.join(summary_dir, output_filename)\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
    "    # Write the overall hypothesis results\n",
    "    all_results_df.to_excel(writer, sheet_name='Hypothesis Results', index=False)\n",
    "    \n",
    "    # Write the overall regression results\n",
    "    overall_results.to_excel(writer, sheet_name='Overall Regression', index=False)\n",
    "\n",
    "    # Apply formatting\n",
    "    workbook = writer.book\n",
    "    header_format = workbook.add_format({\n",
    "        'bold': True, 'text_wrap': True, 'align': 'center', 'valign': 'center', 'bg_color': '#D9EAD3'\n",
    "    })\n",
    "    for sheet_name in writer.sheets:\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "        worksheet.set_row(0, None, header_format)\n",
    "        worksheet.set_column('A:G', 20)  # Adjust column width for better readability\n",
    "\n",
    "    print(f\"Summary statistics and regression results saved to {output_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom X variable List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Parameters\n",
    "segment_results = 'Y'  # Set to 'Y' to segment results by a specific column or 'N' to run the model on the entire dataset\n",
    "segmentation_column = 'Generation'  # The column used for segmentation\n",
    "filter_column = None  # The column used for filtering\n",
    "filter_values = None  # List of values to exclude from the dataset\n",
    "pvalue_threshold = 0.1  # P-value threshold for significance\n",
    "y_column = 'Yvar_USE_AI_Work'  # Dependent variable\n",
    "x_column_prefixes = ['VAR07_JOB_Postive_Import',\n",
    "'VAR08_JOB_Enchance_Job_Security',\n",
    "'VAR09_WORKFORCE_AI_Job_loss',\n",
    "'VAR10_WORKFORCE_AI_Increase_Opportunity_Growth',\n",
    "'VAR14_SAFETY_AI_Need_Protocols',\n",
    "'VAR15_SAFETY_AI_protect_Cyber_Threats']  # List of prefixes for independent variables\n",
    "output_filename_template = 'Regression_Results_{segmentation_column}_Detailed.xlsx'  # Template for output filename\n",
    "\n",
    "# Define the path to your dataset\n",
    "current_directory = os.getcwd()\n",
    "excel_path = os.path.join(current_directory, '01-data', 'TAM_DEF.xlsx')\n",
    "summary_dir = os.path.join(current_directory, '04-summary')\n",
    "\n",
    "# Ensure the summary directory exists\n",
    "if not os.path.exists(summary_dir):\n",
    "    os.makedirs(summary_dir)\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found. Please check the file path: {excel_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error during dataset loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Apply filtering if filter_column and filter_values are set\n",
    "if filter_column and filter_values:\n",
    "    df = df[~df[filter_column].isin(filter_values)]\n",
    "    print(f\"Filtered dataset to exclude {filter_values} in {filter_column}\")\n",
    "\n",
    "if segment_results == 'Y':\n",
    "    # Get unique values in the segmentation column\n",
    "    segments = df[segmentation_column].unique()\n",
    "    print(f\"Found {segmentation_column} segments: {segments}\")\n",
    "else:\n",
    "    # If not segmenting, treat the entire dataset as a single segment\n",
    "    segments = ['Entire Dataset']\n",
    "    df['Entire Dataset'] = 'Entire Dataset'  # Add a dummy column to facilitate the loop\n",
    "\n",
    "# Loop through each segment and run the regression model\n",
    "all_results = []\n",
    "top_variables_set = set()\n",
    "\n",
    "for segment in segments:\n",
    "    df_segment = df[df[segmentation_column] == segment] if segment != 'Entire Dataset' else df\n",
    "    \n",
    "    # Define X (independent variables) and y (dependent variable)\n",
    "    X = pd.concat([df_segment.filter(regex=f'^{prefix}') for prefix in x_column_prefixes], axis=1)\n",
    "    y = df_segment[y_column]\n",
    "    \n",
    "    # Add a constant (intercept) to the model\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Fit the OLS model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Extract the results, filtering by p-value threshold\n",
    "    significant_results = model.summary2().tables[1]\n",
    "    significant_results = significant_results[significant_results['P>|t|'] <= pvalue_threshold]\n",
    "    \n",
    "    # Exclude the constant from the top variables set\n",
    "    top_variables_set.update(var for var in significant_results.index.tolist() if var != 'const')\n",
    "    \n",
    "    # Append results to the list for later export\n",
    "    segment_results = significant_results.reset_index()\n",
    "    segment_results.insert(0, 'Segment', segment)\n",
    "    segment_results.insert(1, 'R2 Value', model.rsquared)\n",
    "    all_results.append(segment_results)\n",
    "\n",
    "# Combine the results from all segments into a DataFrame\n",
    "all_results_df = pd.concat(all_results)\n",
    "\n",
    "# Run overall regression with combined top variables\n",
    "combined_X = df[list(top_variables_set)]\n",
    "combined_X = sm.add_constant(combined_X)\n",
    "combined_model = sm.OLS(df[y_column], combined_X).fit()\n",
    "\n",
    "# Prepare overall model results for export\n",
    "overall_results = combined_model.summary2().tables[1].reset_index()\n",
    "overall_results.insert(0, 'Segment', 'Overall')\n",
    "overall_results.insert(1, 'R2 Value', combined_model.rsquared)\n",
    "\n",
    "# Save the results to Excel, including the segmentation column name in the filename\n",
    "output_filename = output_filename_template.format(segmentation_column=segmentation_column)\n",
    "output_path = os.path.join(summary_dir, output_filename)\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
    "    # Write the overall hypothesis results\n",
    "    all_results_df.to_excel(writer, sheet_name='Hypothesis Results', index=False)\n",
    "    \n",
    "    # Write the overall regression results\n",
    "    overall_results.to_excel(writer, sheet_name='Overall Regression', index=False)\n",
    "\n",
    "    # Apply formatting\n",
    "    workbook = writer.book\n",
    "    header_format = workbook.add_format({\n",
    "        'bold': True, 'text_wrap': True, 'align': 'center', 'valign': 'center', 'bg_color': '#D9EAD3'\n",
    "    })\n",
    "    for sheet_name in writer.sheets:\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "        worksheet.set_row(0, None, header_format)\n",
    "        worksheet.set_column('A:G', 20)  # Adjust column width for better readability\n",
    "\n",
    "    print(f\"Summary statistics and regression results saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Define the nodes and their positions for a left-to-right layout\n",
    "nodes = {\n",
    "    \"Training Opportunities\": (0, 10),\n",
    "    \"Career Growth Opportunities\": (0, 9),\n",
    "    \"Explainability\": (0, 8),\n",
    "    \"Perceived Fairness\": (0, 7),\n",
    "    \"Ethical Considerations\": (0, 6),\n",
    "    \"AI-Driven Personalization\": (0, 5),\n",
    "    \"Ease of Use\": (5, 5),\n",
    "    \"Trust in AI\": (10, 5),\n",
    "    \"Likelihood of AI Usage at Work\": (15, 7)\n",
    "}\n",
    "\n",
    "# Define the edges based on the new relationships\n",
    "edges = [\n",
    "    # Hypothesis 1: Training and Career Growth Opportunities Influence Ease of Use and Trust\n",
    "    (\"Training Opportunities\", \"Ease of Use\"),\n",
    "    (\"Career Growth Opportunities\", \"Ease of Use\"),\n",
    "    (\"Training Opportunities\", \"Trust in AI\"),\n",
    "    (\"Career Growth Opportunities\", \"Trust in AI\"),\n",
    "    \n",
    "    # Hypothesis 2: Explainability and Fairness Influence User Trust and AI Adoption\n",
    "    (\"Explainability\", \"Trust in AI\"),\n",
    "    (\"Perceived Fairness\", \"Trust in AI\"),\n",
    "    \n",
    "    # Hypothesis 3: Ethical Development and Social Responsibility Influence User Trust\n",
    "    (\"Ethical Considerations\", \"Trust in AI\"),\n",
    "    \n",
    "    # Hypothesis 4: AI-Driven Personalization Enhances User Experience\n",
    "    (\"AI-Driven Personalization\", \"Ease of Use\"),\n",
    "    (\"Ease of Use\", \"Trust in AI\"),\n",
    "    (\"Ease of Use\", \"Likelihood of AI Usage at Work\"),\n",
    "    \n",
    "    # Hypothesis 5: Training Accessibility and Accountability Influence User Trust and Usage\n",
    "    (\"Training Opportunities\", \"Trust in AI\"),\n",
    "    (\"Career Growth Opportunities\", \"Trust in AI\"),\n",
    "    (\"Trust in AI\", \"Likelihood of AI Usage at Work\")\n",
    "]\n",
    "\n",
    "# Add nodes and edges to the graph\n",
    "G.add_nodes_from(nodes.keys())\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Define the positions of the nodes\n",
    "pos = {node: (x, y) for node, (x, y) in nodes.items()}\n",
    "\n",
    "# Draw the graph with a larger figure size and increased font size for labels\n",
    "plt.figure(figsize=(25, 12))  # Increase the size here\n",
    "nx.draw_networkx(G, pos, with_labels=True, node_size=3000, node_color=\"lightblue\", font_size=12, font_weight=\"bold\", arrows=True, arrowstyle=\"->\", arrowsize=15)\n",
    "plt.title(\"A conceptual diagram illustrating the relationship between AI-related factors and user trust/adoption\", fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Filtered dataset to exclude ['Boomer'] in Generation\n",
      "\n",
      "Model created and dataset loaded into the model successfully for Entire Dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Fisher Information Matrix is not PD.Moore-Penrose inverse will be used instead of Cholesky decomposition. See 10.1109/TSP.2012.2208105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model optimization completed successfully for Entire Dataset.\n",
      "\n",
      "Full Results DataFrame for Entire Dataset:\n",
      "                                  lval  op  \\\n",
      "0        VAR11_PRIVACY_AI_Protect_Data   ~   \n",
      "1    VAR16_ETHICS_AI_Developed_Ethical   ~   \n",
      "2    VAR25_FAIRNESS_AI_Treats_All_Fair   ~   \n",
      "3    VAR26_FAIRNESS_Should_Reduce_Bias   ~   \n",
      "4                                Trust   ~   \n",
      "5                                Trust   ~   \n",
      "6                                Trust   ~   \n",
      "7                                Trust   ~   \n",
      "8                                Trust   ~   \n",
      "9                                Trust   ~   \n",
      "10                               Trust   ~   \n",
      "11  VAR28_PERSONAL_Enhances_Experience   ~   \n",
      "12   VAR29_PERONAL_Improves_CS_quality   ~   \n",
      "13      VAR21_EXPLAIN_clear_descisions   ~   \n",
      "14         VAR06_ED_AI_Training_needed   ~   \n",
      "15         VAR03_CG_AI_Training_Access   ~   \n",
      "16                   VAR01_CG_Training   ~   \n",
      "17   VAR04_CG_AI_Training_helps_skills   ~   \n",
      "18  VAR19_ACCOUNTABILITY_AI_Mechanisms   ~   \n",
      "19                    Yvar_USE_AI_Work   ~   \n",
      "20                    Yvar_USE_AI_Work   ~   \n",
      "21                    Yvar_USE_AI_Work   ~   \n",
      "22                    Yvar_USE_AI_Work   ~   \n",
      "23                         Ease_of_Use  ~~   \n",
      "24                         Ease_of_Use  ~~   \n",
      "25                         Ease_of_Use  ~~   \n",
      "26                            Training  ~~   \n",
      "27                               Trust  ~~   \n",
      "28       VAR11_PRIVACY_AI_Protect_Data  ~~   \n",
      "29   VAR16_ETHICS_AI_Developed_Ethical  ~~   \n",
      "30   VAR25_FAIRNESS_AI_Treats_All_Fair  ~~   \n",
      "31   VAR26_FAIRNESS_Should_Reduce_Bias  ~~   \n",
      "32                              ethics  ~~   \n",
      "33                              ethics  ~~   \n",
      "34                   VAR01_CG_Training  ~~   \n",
      "35         VAR03_CG_AI_Training_Access  ~~   \n",
      "36   VAR04_CG_AI_Training_helps_skills  ~~   \n",
      "37         VAR06_ED_AI_Training_needed  ~~   \n",
      "38  VAR19_ACCOUNTABILITY_AI_Mechanisms  ~~   \n",
      "39      VAR21_EXPLAIN_clear_descisions  ~~   \n",
      "40  VAR28_PERSONAL_Enhances_Experience  ~~   \n",
      "41   VAR29_PERONAL_Improves_CS_quality  ~~   \n",
      "42                    Yvar_USE_AI_Work  ~~   \n",
      "\n",
      "                                         rval  Estimate  Std. Err    z-value  \\\n",
      "0                                       Trust  1.000000       NaN        NaN   \n",
      "1                                       Trust  0.000000  0.002780   0.000000   \n",
      "2                                       Trust  0.000000  0.002098   0.000000   \n",
      "3                                       Trust  0.000000  0.003252   0.000000   \n",
      "4                                    Training  0.000000  0.338666   0.000000   \n",
      "5           VAR16_ETHICS_AI_Developed_Ethical  0.000000  0.036165   0.000000   \n",
      "6       VAR15_SAFETY_AI_protect_Cyber_Threats  0.000000  0.030387   0.000000   \n",
      "7   VAR17_ETHICS_AI_proritize_Human_Wellbeing  0.000000  0.029819   0.000000   \n",
      "8               VAR11_PRIVACY_AI_Protect_Data  0.000000  0.013499   0.000000   \n",
      "9           VAR25_FAIRNESS_AI_Treats_All_Fair  0.000000  0.032966   0.000000   \n",
      "10          VAR26_FAIRNESS_Should_Reduce_Bias  0.000000  0.038070   0.000000   \n",
      "11                                Ease_of_Use  1.000000       NaN        NaN   \n",
      "12                                Ease_of_Use  0.683023  1.286320   0.530990   \n",
      "13                                Ease_of_Use  0.093074  0.298715   0.311581   \n",
      "14                                   Training  1.000000       NaN        NaN   \n",
      "15                                   Training  0.581788  0.543445   1.070555   \n",
      "16                                   Training  0.162940  0.366648   0.444405   \n",
      "17                                   Training  0.538976  0.502113   1.073416   \n",
      "18                                     ethics  1.000000       NaN        NaN   \n",
      "19                                      Trust  0.218517  0.056220   3.886827   \n",
      "20                                Ease_of_Use  0.490592  0.862905   0.568535   \n",
      "21                                   Training  0.510927  0.545548   0.936540   \n",
      "22                                     ethics  0.023116  0.409565   0.056441   \n",
      "23                                Ease_of_Use  0.050000  0.099953   0.500235   \n",
      "24                                   Training  0.000000  0.013797   0.000000   \n",
      "25                                     ethics  0.000000  0.015305   0.000000   \n",
      "26                                   Training  0.050000  0.051613   0.968740   \n",
      "27                                      Trust  0.050000  0.117850   0.424267   \n",
      "28              VAR11_PRIVACY_AI_Protect_Data  0.689109  0.128728   5.353205   \n",
      "29          VAR16_ETHICS_AI_Developed_Ethical  0.650412  0.031908  20.383817   \n",
      "30          VAR25_FAIRNESS_AI_Treats_All_Fair  0.785648  0.038543  20.383817   \n",
      "31          VAR26_FAIRNESS_Should_Reduce_Bias  0.585317  0.028715  20.383817   \n",
      "32                                     ethics  0.050000  0.095103   0.525745   \n",
      "33                                   Training  0.000000  0.012406   0.000000   \n",
      "34                          VAR01_CG_Training  0.828492  0.040979  20.217622   \n",
      "35                VAR03_CG_AI_Training_Access  0.436468  0.028087  15.539872   \n",
      "36          VAR04_CG_AI_Training_helps_skills  0.461939  0.027807  16.612340   \n",
      "37                VAR06_ED_AI_Training_needed  0.447454  0.055065   8.125986   \n",
      "38         VAR19_ACCOUNTABILITY_AI_Mechanisms  0.346735  0.095247   3.640388   \n",
      "39             VAR21_EXPLAIN_clear_descisions  0.337240  0.016717  20.173706   \n",
      "40         VAR28_PERSONAL_Enhances_Experience  0.636774  0.103958   6.125310   \n",
      "41          VAR29_PERONAL_Improves_CS_quality  0.692925  0.057554  12.039492   \n",
      "42                           Yvar_USE_AI_Work  0.896697  0.054389  16.486801   \n",
      "\n",
      "         p-value  \n",
      "0            NaN  \n",
      "1   1.000000e+00  \n",
      "2   1.000000e+00  \n",
      "3   1.000000e+00  \n",
      "4   1.000000e+00  \n",
      "5   1.000000e+00  \n",
      "6   1.000000e+00  \n",
      "7   1.000000e+00  \n",
      "8   1.000000e+00  \n",
      "9   1.000000e+00  \n",
      "10  1.000000e+00  \n",
      "11           NaN  \n",
      "12  5.954255e-01  \n",
      "13  7.553587e-01  \n",
      "14           NaN  \n",
      "15  2.843696e-01  \n",
      "16  6.567499e-01  \n",
      "17  2.830846e-01  \n",
      "18           NaN  \n",
      "19  1.015633e-04  \n",
      "20  5.696715e-01  \n",
      "21  3.489950e-01  \n",
      "22  9.549905e-01  \n",
      "23  6.169096e-01  \n",
      "24  1.000000e+00  \n",
      "25  1.000000e+00  \n",
      "26  3.326748e-01  \n",
      "27  6.713710e-01  \n",
      "28  8.641004e-08  \n",
      "29  0.000000e+00  \n",
      "30  0.000000e+00  \n",
      "31  0.000000e+00  \n",
      "32  5.990653e-01  \n",
      "33  1.000000e+00  \n",
      "34  0.000000e+00  \n",
      "35  0.000000e+00  \n",
      "36  0.000000e+00  \n",
      "37  4.440892e-16  \n",
      "38  2.722271e-04  \n",
      "39  0.000000e+00  \n",
      "40  9.050702e-10  \n",
      "41  0.000000e+00  \n",
      "42  0.000000e+00  \n",
      "\n",
      "P-values extracted successfully for Entire Dataset:\n",
      "         p-value\n",
      "0            NaN\n",
      "1   1.000000e+00\n",
      "2   1.000000e+00\n",
      "3   1.000000e+00\n",
      "4   1.000000e+00\n",
      "5   1.000000e+00\n",
      "6   1.000000e+00\n",
      "7   1.000000e+00\n",
      "8   1.000000e+00\n",
      "9   1.000000e+00\n",
      "10  1.000000e+00\n",
      "11           NaN\n",
      "12  5.954255e-01\n",
      "13  7.553587e-01\n",
      "14           NaN\n",
      "15  2.843696e-01\n",
      "16  6.567499e-01\n",
      "17  2.830846e-01\n",
      "18           NaN\n",
      "19  1.015633e-04\n",
      "20  5.696715e-01\n",
      "21  3.489950e-01\n",
      "22  9.549905e-01\n",
      "23  6.169096e-01\n",
      "24  1.000000e+00\n",
      "25  1.000000e+00\n",
      "26  3.326748e-01\n",
      "27  6.713710e-01\n",
      "28  8.641004e-08\n",
      "29  0.000000e+00\n",
      "30  0.000000e+00\n",
      "31  0.000000e+00\n",
      "32  5.990653e-01\n",
      "33  1.000000e+00\n",
      "34  0.000000e+00\n",
      "35  0.000000e+00\n",
      "36  0.000000e+00\n",
      "37  4.440892e-16\n",
      "38  2.722271e-04\n",
      "39  0.000000e+00\n",
      "40  9.050702e-10\n",
      "41  0.000000e+00\n",
      "42  0.000000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ft/dwmcjbnd0b35z5n9yksp4jfh0000gn/T/ipykernel_20965/442535573.py:105: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  results = results.applymap(lambda x: np.nan if x in [\"Not estimated\", \"-\", None] else x)\n",
      "WARNING:root:Fisher Information Matrix is not PD.Moore-Penrose inverse will be used instead of Cholesky decomposition. See 10.1109/TSP.2012.2208105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Hypothesis Results:\n",
      "          Segment                                         Hypothesis  \\\n",
      "0  Entire Dataset  Hypothesis 1: AI Training and Career Growth Op...   \n",
      "1  Entire Dataset  Hypothesis 2: Explainability and Fairness Infl...   \n",
      "2  Entire Dataset  Hypothesis 3: Ethical Development and Social R...   \n",
      "3  Entire Dataset  Hypothesis 4: AI-Driven Personalization Enhanc...   \n",
      "4  Entire Dataset  Hypothesis 5: AI Training Accessibility and Ac...   \n",
      "\n",
      "    p-value  Estimate  Std. Err   z-value    Result  \n",
      "0  0.348995  0.510927  0.545548  0.936540  Rejected  \n",
      "1  0.000102  0.218517  0.056220  3.886827  Accepted  \n",
      "2  0.954990  0.023116  0.409565  0.056441  Rejected  \n",
      "3  0.569671  0.490592  0.862905  0.568535  Rejected  \n",
      "4  0.569671  0.490592  0.862905  0.568535  Rejected  \n",
      "\n",
      "SEM results and hypothesis results saved to /Users/danramirez/mbs-structural-equation-modeling/04-summary/SEM_Results.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from semopy import Model, Optimizer\n",
    "\n",
    "# Parameters\n",
    "p_value_threshold = 0.05  # Set the threshold for p-value\n",
    "dependent_variable = 'Yvar_USE_AI_Work'  # Set the dependent variable\n",
    "segment_results = 'N'  # 'Y' or 'N'\n",
    "segmentation_column = 'Roles'  # The column used for segmentation if segment_results = 'Y'\n",
    "filter_column = 'Generation'  # The column used for filtering\n",
    "filter_values = 'Boomer'  # List of values to exclude from the dataset\n",
    "\n",
    "# Define the path to your dataset\n",
    "current_directory = os.getcwd()\n",
    "excel_path = os.path.join(current_directory, '01-data', 'TAM_DEF.xlsx')\n",
    "summary_dir = os.path.join(current_directory, '04-summary')\n",
    "\n",
    "# Ensure the summary directory exists\n",
    "if not os.path.exists(summary_dir):\n",
    "    os.makedirs(summary_dir)\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found. Please check the file path: {excel_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error during dataset loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Apply filtering if filter_column and filter_values are set\n",
    "if filter_column and filter_values is not None:\n",
    "    # Ensure filter_values is a list\n",
    "    if not isinstance(filter_values, (list, tuple, np.ndarray)):\n",
    "        filter_values = [filter_values]\n",
    "    \n",
    "    df = df[~df[filter_column].isin(filter_values)]\n",
    "    print(f\"Filtered dataset to exclude {filter_values} in {filter_column}\")\n",
    "\n",
    "if segment_results == 'Y':\n",
    "    # Get unique values in the segmentation column\n",
    "    segments = df[segmentation_column].unique()\n",
    "    print(f\"Found {segmentation_column} segments: {segments}\")\n",
    "else:\n",
    "    # If not segmenting, treat the entire dataset as a single segment\n",
    "    segments = ['Entire Dataset']\n",
    "    df['Entire Dataset'] = 'Entire Dataset'  # Add a dummy column to facilitate the loop\n",
    "\n",
    "# Initialize a list to store hypothesis results for all segments\n",
    "all_hypothesis_results = []\n",
    "\n",
    "# Loop through each segment and run the SEM model\n",
    "for segment in segments:\n",
    "    df_segment = df[df[segmentation_column] == segment] if segment_results == 'Y' else df\n",
    "    \n",
    "    # Define the SEM model\n",
    "    model_desc = f\"\"\"\n",
    "    # Latent variables\n",
    "    Trust =~ VAR11_PRIVACY_AI_Protect_Data + VAR16_ETHICS_AI_Developed_Ethical + VAR25_FAIRNESS_AI_Treats_All_Fair + VAR26_FAIRNESS_Should_Reduce_Bias\n",
    "    Ease_of_Use =~  VAR28_PERSONAL_Enhances_Experience + VAR29_PERONAL_Improves_CS_quality + VAR21_EXPLAIN_clear_descisions\n",
    "    Training =~ VAR06_ED_AI_Training_needed + VAR03_CG_AI_Training_Access + VAR01_CG_Training + VAR04_CG_AI_Training_helps_skills\n",
    "    ethics =~ VAR19_ACCOUNTABILITY_AI_Mechanisms\n",
    "    \n",
    "    # Direct relationships with Usage ({dependent_variable})\n",
    "    {dependent_variable} ~ Trust\n",
    "    {dependent_variable} ~ Ease_of_Use\n",
    "    {dependent_variable} ~ Training\n",
    "    {dependent_variable} ~ ethics\n",
    "\n",
    "    # Relationships with latent variables\n",
    "    Trust ~ Training\n",
    "    Trust ~ VAR16_ETHICS_AI_Developed_Ethical\n",
    "    Trust ~ VAR15_SAFETY_AI_protect_Cyber_Threats\n",
    "    Trust ~ VAR17_ETHICS_AI_proritize_Human_Wellbeing\n",
    "    Trust ~ VAR11_PRIVACY_AI_Protect_Data\n",
    "    Trust ~ VAR25_FAIRNESS_AI_Treats_All_Fair\n",
    "    Trust ~ VAR26_FAIRNESS_Should_Reduce_Bias\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the model and load the dataset into the model\n",
    "    try:\n",
    "        model = Model(model_desc)\n",
    "        model.load_dataset(df_segment)\n",
    "        print(f\"\\nModel created and dataset loaded into the model successfully for {segment}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model creation or dataset loading for {segment}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Optimize the model\n",
    "    try:\n",
    "        optim = Optimizer(model)\n",
    "        optim.optimize()\n",
    "        print(f\"Model optimization completed successfully for {segment}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model optimization for {segment}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Extract the results\n",
    "    try:\n",
    "        results = model.inspect()\n",
    "        # Convert any \"Not estimated\" or non-numeric values to NaN\n",
    "        results = results.applymap(lambda x: np.nan if x in [\"Not estimated\", \"-\", None] else x)\n",
    "        print(f\"\\nFull Results DataFrame for {segment}:\")\n",
    "        print(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during results extraction for {segment}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Attempt to extract p-values for the paths\n",
    "    try:\n",
    "        pvalues = results[['p-value']].apply(pd.to_numeric, errors='coerce')  # Convert to numeric, set errors to NaN\n",
    "        print(f\"\\nP-values extracted successfully for {segment}:\")\n",
    "        print(pvalues)\n",
    "    except KeyError:\n",
    "        print(f\"\\nUnable to extract p-values for {segment}. Check the results DataFrame above for available data.\")\n",
    "        continue\n",
    "\n",
    "    # Define hypotheses and their corresponding paths based on the five hypotheses\n",
    "    hypothesis_criteria = [\n",
    "        (f\"Hypothesis 1: AI Training and Career Growth Opportunities Influence Ease of Use and Trust\", f'{dependent_variable} ~ Training'),\n",
    "        (f\"Hypothesis 2: Explainability and Fairness Influence User Trust and AI Adoption\", f'{dependent_variable} ~ Trust'),\n",
    "        (f\"Hypothesis 3: Ethical Development and Social Responsibility Influence User Use\", f'{dependent_variable} ~ ethics'),\n",
    "        (f\"Hypothesis 4: AI-Driven Personalization Enhances User Experience\", f'{dependent_variable} ~ Ease_of_Use'),\n",
    "        (f\"Hypothesis 5: AI Training Accessibility and Accountability Influence User Trust \", f'{dependent_variable} ~ Ease_of_Use')\n",
    "    ]\n",
    "\n",
    "    # Create a DataFrame to store the hypothesis results for the current segment\n",
    "    hypothesis_results = []\n",
    "\n",
    "    # Determine whether each hypothesis is accepted or rejected\n",
    "    for hyp, path in hypothesis_criteria:\n",
    "        matching_paths = results[(results['lval'] == path.split(' ~ ')[0]) & \n",
    "                                 (results['rval'] == path.split(' ~ ')[1])]\n",
    "        if not matching_paths.empty:\n",
    "            p_value = matching_paths['p-value'].values[0]\n",
    "            estimate = matching_paths['Estimate'].values[0]\n",
    "            std_err = matching_paths['Std. Err'].values[0]\n",
    "            z_value = matching_paths['z-value'].values[0]\n",
    "            result = 'Accepted' if not np.isnan(p_value) and p_value < p_value_threshold else 'Rejected'\n",
    "            hypothesis_results.append({\n",
    "                'Segment': segment,\n",
    "                'Hypothesis': hyp,\n",
    "                'p-value': p_value,\n",
    "                'Estimate': estimate,\n",
    "                'Std. Err': std_err,\n",
    "                'z-value': z_value,\n",
    "                'Result': result\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Path {path} not found in results for {segment}. Please check the available paths.\")\n",
    "            hypothesis_results.append({\n",
    "                'Segment': segment,\n",
    "                'Hypothesis': hyp,\n",
    "                'p-value': np.nan,\n",
    "                'Estimate': np.nan,\n",
    "                'Std. Err': np.nan,\n",
    "                'z-value': np.nan,\n",
    "                'Result': 'Path Not Found'\n",
    "            })\n",
    "\n",
    "    # Append the current segment's hypothesis results to the overall list\n",
    "    all_hypothesis_results.extend(hypothesis_results)\n",
    "\n",
    "# Convert the overall hypothesis results to a DataFrame\n",
    "all_hypothesis_df = pd.DataFrame(all_hypothesis_results)\n",
    "\n",
    "# Save the results to Excel\n",
    "output_path = os.path.join(summary_dir, 'SEM_Results.xlsx')\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    # Write the overall hypothesis results\n",
    "    all_hypothesis_df.to_excel(writer, sheet_name='Hypothesis Results', index=False)\n",
    "    \n",
    "    # Loop through each segment and write the SEM results for each segment in a separate tab\n",
    "    for segment in segments:\n",
    "        df_segment_results = df[df[segmentation_column] == segment] if segment_results == 'Y' else df\n",
    "        # Extract the SEM results for the segment\n",
    "        model = Model(model_desc)\n",
    "        model.load_dataset(df_segment_results)\n",
    "        optim = Optimizer(model)\n",
    "        optim.optimize()\n",
    "        results = model.inspect()\n",
    "        # Write SEM results to its own sheet\n",
    "        results.to_excel(writer, sheet_name=f'SEM Results - {segment}')\n",
    "\n",
    "# Print the final hypothesis results\n",
    "print(\"\\nFinal Hypothesis Results:\")\n",
    "print(all_hypothesis_df)\n",
    "\n",
    "print(f\"\\nSEM results and hypothesis results saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEM Bootscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from semopy import Model, Optimizer\n",
    "\n",
    "# Parameters\n",
    "p_value_threshold = 0.1  # Set the threshold for p-value\n",
    "dependent_variable = 'Yvar_USE_AI_Work'  # Set the dependent variable\n",
    "segment_results = 'N'  # 'Y' or 'N'\n",
    "segmentation_column = 'Generation'  # The column used for segmentation if segment_results = 'Y'\n",
    "filter_column = None  # The column used for filtering\n",
    "filter_values = [None]  # List of values to exclude from the dataset\n",
    "n_bootstrap = 500  # Number of bootstrap samples for testing\n",
    "\n",
    "# Define the path to your dataset\n",
    "current_directory = os.getcwd()\n",
    "excel_path = os.path.join(current_directory, '01-data', 'TAM_DEF.xlsx')\n",
    "summary_dir = os.path.join(current_directory, '04-summary')\n",
    "\n",
    "# Ensure the summary directory exists\n",
    "if not os.path.exists(summary_dir):\n",
    "    os.makedirs(summary_dir)\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found. Please check the file path: {excel_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error during dataset loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Apply filtering if filter_column and filter_values are set\n",
    "if filter_column and filter_values:\n",
    "    df = df[~df[filter_column].isin(filter_values)]\n",
    "    print(f\"Filtered dataset to exclude {filter_values} in {filter_column}\")\n",
    "\n",
    "if segment_results == 'Y':\n",
    "    # Get unique values in the segmentation column\n",
    "    segments = df[segmentation_column].unique()\n",
    "    print(f\"Found {segmentation_column} segments: {segments}\")\n",
    "else:\n",
    "    # If not segmenting, treat the entire dataset as a single segment\n",
    "    segments = ['Entire Dataset']\n",
    "    df['Entire Dataset'] = 'Entire Dataset'  # Add a dummy column to facilitate the loop\n",
    "\n",
    "# Function to run the model and get results\n",
    "def run_model_and_get_results(df_segment):\n",
    "    model_desc = f\"\"\"\n",
    "    # Latent variables\n",
    "    Trust =~ VAR11_PRIVACY_AI_Protect_Data + VAR16_ETHICS_AI_Developed_Ethical + VAR25_FAIRNESS_AI_Treats_All_Fair + VAR26_FAIRNESS_Should_Reduce_Bias\n",
    "    Ease_of_Use =~  VAR28_PERSONAL_Enhances_Experience + VAR29_PERONAL_Improves_CS_quality\n",
    "    Training =~ VAR05_CG_AI_Training_Supported + VAR03_CG_AI_Training_Access + VAR01_CG_Training +VAR04_CG_AI_Training_helps_skills\n",
    "    \n",
    "    # Direct relationships with Usage ({dependent_variable})\n",
    "    {dependent_variable} ~ Trust\n",
    "    {dependent_variable} ~ Ease_of_Use\n",
    "    {dependent_variable} ~ Training\n",
    "\n",
    "    # Relationships with latent variables\n",
    "    Trust ~ Training\n",
    "    Trust ~ VAR16_ETHICS_AI_Developed_Ethical\n",
    "    Trust ~ VAR15_SAFETY_AI_protect_Cyber_Threats\n",
    "    Trust ~ VAR17_ETHICS_AI_proritize_Human_Wellbeing\n",
    "    Trust ~ VAR11_PRIVACY_AI_Protect_Data\n",
    "    Trust ~ VAR25_FAIRNESS_AI_Treats_All_Fair\n",
    "    Trust ~ VAR26_FAIRNESS_Should_Reduce_Bias\n",
    "    \"\"\"\n",
    "    model = Model(model_desc)\n",
    "    model.load_dataset(df_segment)\n",
    "    optim = Optimizer(model)\n",
    "    optim.optimize()\n",
    "    results = model.inspect()\n",
    "    return results\n",
    "#---------------------------------\n",
    "\n",
    "# Run non-bootstrapped model and get results\n",
    "try:\n",
    "    non_bootstrap_results = run_model_and_get_results(df)\n",
    "    print(\"Non-Bootstrap Results:\")\n",
    "    #print(non_bootstrap_results)\n",
    "except Exception as e:\n",
    "    print(f\"Error during non-bootstrapped model creation: {e}\")\n",
    "\n",
    "# Initialize list to hold bootstrapped results\n",
    "bootstrap_estimates = []\n",
    "\n",
    "# Bootstrapping\n",
    "for i in range(n_bootstrap):\n",
    "    print(f\"\\nProcessing bootstrap sample {i+1}/{n_bootstrap} for segment: Entire Dataset\")\n",
    "    bootstrap_sample = df.sample(n=len(df), replace=True)\n",
    "    \n",
    "    #print(f\"Bootstrap Sample {i+1} Head:\\n\", bootstrap_sample.head())\n",
    "    \n",
    "    try:\n",
    "        bootstrap_results = run_model_and_get_results(bootstrap_sample)\n",
    "        #print(f\"Bootstrap {i+1} results:\\n\", bootstrap_results[['Estimate', 'p-value']])\n",
    "        \n",
    "        # Convert all relevant columns to numeric, coercing errors\n",
    "        bootstrap_results[['Estimate', 'Std. Err', 'z-value', 'p-value']] = bootstrap_results[['Estimate', 'Std. Err', 'z-value', 'p-value']].apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        bootstrap_estimates.append(bootstrap_results[['lval', 'rval', 'Estimate', 'Std. Err', 'z-value', 'p-value']])\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model creation or optimization for bootstrap sample {i+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Aggregating bootstrapped results\n",
    "if bootstrap_estimates:\n",
    "    bootstrap_df = pd.concat(bootstrap_estimates)\n",
    "    bootstrap_df = bootstrap_df.groupby(['lval', 'rval']).mean(numeric_only=True)\n",
    "\n",
    "    # Prepare final results for hypothesis testing\n",
    "    hypothesis_results = []\n",
    "    hypothesis_criteria = {\n",
    "        'H 1: AI Training and Career Growth Opportunities Influence Ease of Use and Trust': (dependent_variable, 'Training'),\n",
    "        'H 2: Explainability and Fairness Influence User Trust and AI Adoption': (dependent_variable, 'Trust'),\n",
    "        'H 3: Ethical Development and Social Responsibility Influence User Trust': ('Trust', 'VAR17_ETHICS_AI_proritize_Human_Wellbeing'),\n",
    "        'H 4: AI-Driven Personalization Enhances User Experience': (dependent_variable, 'Ease_of_Use'),\n",
    "        'H 5: AI Training Accessibility and Accountability Influence User Trust and Usage': ('Trust', 'Training')\n",
    "    }\n",
    "\n",
    "    for hypothesis, (lval, rval) in hypothesis_criteria.items():\n",
    "        if (lval, rval) in bootstrap_df.index:\n",
    "            p_value = bootstrap_df.loc[(lval, rval), 'p-value']\n",
    "            result = 'Accepted' if p_value < p_value_threshold else 'Rejected'\n",
    "            hypothesis_results.append({\n",
    "                'Hypothesis': hypothesis,\n",
    "                'p-value': p_value,\n",
    "                'Estimate': bootstrap_df.loc[(lval, rval), 'Estimate'],\n",
    "                'Std. Err': bootstrap_df.loc[(lval, rval), 'Std. Err'],\n",
    "                'z-value': bootstrap_df.loc[(lval, rval), 'z-value'],\n",
    "                'Result': result\n",
    "            })\n",
    "        else:\n",
    "            hypothesis_results.append({\n",
    "                'Hypothesis': hypothesis,\n",
    "                'p-value': np.nan,\n",
    "                'Estimate': np.nan,\n",
    "                'Std. Err': np.nan,\n",
    "                'z-value': np.nan,\n",
    "                'Result': 'Error'\n",
    "            })\n",
    "\n",
    "    # Convert hypothesis results to a DataFrame\n",
    "    hypothesis_results_df = pd.DataFrame(hypothesis_results)\n",
    "\n",
    "    print(\"\\nHypothesis Testing Results:\")\n",
    "    print(hypothesis_results_df)\n",
    "\n",
    "    # Save the results to Excel\n",
    "    output_path = os.path.join(summary_dir, 'SEM_Results_Bootstrap_Final.xlsx')\n",
    "    with pd.ExcelWriter(output_path) as writer:\n",
    "        # Save non-bootstrapped results\n",
    "        non_bootstrap_results.to_excel(writer, sheet_name='Non-Bootstrap Results', index=False)\n",
    "        \n",
    "        # Save aggregated bootstrap results\n",
    "        bootstrap_df.to_excel(writer, sheet_name='Aggregated Bootstrap Results')\n",
    "        \n",
    "        # Save hypothesis testing results\n",
    "        hypothesis_results_df.to_excel(writer, sheet_name='Hypothesis Testing Results', index=False)\n",
    "        \n",
    "        print(f\"\\nFinal results saved to {output_path}.\")\n",
    "else:\n",
    "    print(\"No valid bootstrap estimates were generated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
