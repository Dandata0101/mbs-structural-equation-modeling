{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exploratory Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Found Generation segments: ['Boomer' 'Gen Z' 'Gen X' 'Millennial']\n",
      "Summary statistics and regression results saved to /Users/danramirez/mbs-structural-equation-modeling/04-summary/SEM_Results_Generation_Detailed.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Parameters\n",
    "segment_results = 'Y'  # Set to 'Y' to segment results by a specific column or 'N' to run the model on the entire dataset\n",
    "segmentation_column = 'Generation'  # The column used for segmentation\n",
    "filter_column = None  # The column used for filtering\n",
    "filter_values = None  # List of values to exclude from the dataset\n",
    "pvalue_threshold = 0.05  # P-value threshold for significance\n",
    "y_column = 'Yvar_USE_AI_Work'  # Dependent variable\n",
    "x_column_prefix = 'VAR'  # Prefix for independent variables\n",
    "output_filename_template = 'Regression_Results_{segmentation_column}_Detailed.xlsx'  # Template for output filename\n",
    "\n",
    "# Define the path to your dataset\n",
    "current_directory = os.getcwd()\n",
    "excel_path = os.path.join(current_directory, '01-data', 'TAM_DEF.xlsx')\n",
    "summary_dir = os.path.join(current_directory, '04-summary')\n",
    "\n",
    "# Ensure the summary directory exists\n",
    "if not os.path.exists(summary_dir):\n",
    "    os.makedirs(summary_dir)\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found. Please check the file path: {excel_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error during dataset loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Apply filtering if filter_column and filter_values are set\n",
    "if filter_column and filter_values:\n",
    "    df = df[~df[filter_column].isin(filter_values)]\n",
    "    print(f\"Filtered dataset to exclude {filter_values} in {filter_column}\")\n",
    "\n",
    "if segment_results == 'Y':\n",
    "    # Get unique values in the segmentation column\n",
    "    segments = df[segmentation_column].unique()\n",
    "    print(f\"Found {segmentation_column} segments: {segments}\")\n",
    "else:\n",
    "    # If not segmenting, treat the entire dataset as a single segment\n",
    "    segments = ['Entire Dataset']\n",
    "    df['Entire Dataset'] = 'Entire Dataset'  # Add a dummy column to facilitate the loop\n",
    "\n",
    "# Loop through each segment and run the regression model\n",
    "all_results = []\n",
    "top_variables_set = set()\n",
    "\n",
    "for segment in segments:\n",
    "    df_segment = df[df[segmentation_column] == segment] if segment != 'Entire Dataset' else df\n",
    "    \n",
    "    # Define X (independent variables) and y (dependent variable)\n",
    "    X = df_segment.filter(regex=f'^{x_column_prefix}')\n",
    "    y = df_segment[y_column]\n",
    "    \n",
    "    # Add a constant (intercept) to the model\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Fit the OLS model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Extract the results, filtering by p-value threshold\n",
    "    significant_results = model.summary2().tables[1]\n",
    "    significant_results = significant_results[significant_results['P>|t|'] <= pvalue_threshold]\n",
    "    \n",
    "    # Exclude the constant from the top variables set\n",
    "    top_variables_set.update(var for var in significant_results.index.tolist() if var != 'const')\n",
    "    \n",
    "    # Append results to the list for later export\n",
    "    segment_results = significant_results.reset_index()\n",
    "    segment_results.insert(0, 'Segment', segment)\n",
    "    segment_results.insert(1, 'R2 Value', model.rsquared)\n",
    "    all_results.append(segment_results)\n",
    "\n",
    "# Combine the results from all segments into a DataFrame\n",
    "all_results_df = pd.concat(all_results)\n",
    "\n",
    "# Run overall regression with combined top variables\n",
    "combined_X = df[list(top_variables_set)]\n",
    "combined_X = sm.add_constant(combined_X)\n",
    "combined_model = sm.OLS(df[y_column], combined_X).fit()\n",
    "\n",
    "# Prepare overall model results for export\n",
    "overall_results = combined_model.summary2().tables[1].reset_index()\n",
    "overall_results.insert(0, 'Segment', 'Overall')\n",
    "overall_results.insert(1, 'R2 Value', combined_model.rsquared)\n",
    "\n",
    "# Save the results to Excel, including the segmentation column name in the filename\n",
    "output_filename = output_filename_template.format(segmentation_column=segmentation_column)\n",
    "output_path = os.path.join(summary_dir, output_filename)\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
    "    # Write the overall hypothesis results\n",
    "    all_results_df.to_excel(writer, sheet_name='Hypothesis Results', index=False)\n",
    "    \n",
    "    # Write the overall regression results\n",
    "    overall_results.to_excel(writer, sheet_name='Overall Regression', index=False)\n",
    "\n",
    "    # Apply formatting\n",
    "    workbook = writer.book\n",
    "    header_format = workbook.add_format({\n",
    "        'bold': True, 'text_wrap': True, 'align': 'center', 'valign': 'center', 'bg_color': '#D9EAD3'\n",
    "    })\n",
    "    for sheet_name in writer.sheets:\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "        worksheet.set_row(0, None, header_format)\n",
    "        worksheet.set_column('A:G', 20)  # Adjust column width for better readability\n",
    "\n",
    "    print(f\"Summary statistics and regression results saved to {output_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Define the nodes and their positions for a left-to-right layout\n",
    "nodes = {\n",
    "    \"Training Opportunities\": (0, 10),\n",
    "    \"Career Growth Opportunities\": (0, 9),\n",
    "    \"Explainability\": (0, 8),\n",
    "    \"Perceived Fairness\": (0, 7),\n",
    "    \"Ethical Considerations\": (0, 6),\n",
    "    \"AI-Driven Personalization\": (0, 5),\n",
    "    \"Ease of Use\": (5, 5),\n",
    "    \"Trust in AI\": (10, 5),\n",
    "    \"Likelihood of AI Usage at Work\": (15, 7)\n",
    "}\n",
    "\n",
    "# Define the edges based on the new relationships\n",
    "edges = [\n",
    "    # Hypothesis 1: Training and Career Growth Opportunities Influence Ease of Use and Trust\n",
    "    (\"Training Opportunities\", \"Ease of Use\"),\n",
    "    (\"Career Growth Opportunities\", \"Ease of Use\"),\n",
    "    (\"Training Opportunities\", \"Trust in AI\"),\n",
    "    (\"Career Growth Opportunities\", \"Trust in AI\"),\n",
    "    \n",
    "    # Hypothesis 2: Explainability and Fairness Influence User Trust and AI Adoption\n",
    "    (\"Explainability\", \"Trust in AI\"),\n",
    "    (\"Perceived Fairness\", \"Trust in AI\"),\n",
    "    \n",
    "    # Hypothesis 3: Ethical Development and Social Responsibility Influence User Trust\n",
    "    (\"Ethical Considerations\", \"Trust in AI\"),\n",
    "    \n",
    "    # Hypothesis 4: AI-Driven Personalization Enhances User Experience\n",
    "    (\"AI-Driven Personalization\", \"Ease of Use\"),\n",
    "    (\"Ease of Use\", \"Trust in AI\"),\n",
    "    (\"Ease of Use\", \"Likelihood of AI Usage at Work\"),\n",
    "    \n",
    "    # Hypothesis 5: Training Accessibility and Accountability Influence User Trust and Usage\n",
    "    (\"Training Opportunities\", \"Trust in AI\"),\n",
    "    (\"Career Growth Opportunities\", \"Trust in AI\"),\n",
    "    (\"Trust in AI\", \"Likelihood of AI Usage at Work\")\n",
    "]\n",
    "\n",
    "# Add nodes and edges to the graph\n",
    "G.add_nodes_from(nodes.keys())\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Define the positions of the nodes\n",
    "pos = {node: (x, y) for node, (x, y) in nodes.items()}\n",
    "\n",
    "# Draw the graph with a larger figure size and increased font size for labels\n",
    "plt.figure(figsize=(25, 12))  # Increase the size here\n",
    "nx.draw_networkx(G, pos, with_labels=True, node_size=3000, node_color=\"lightblue\", font_size=12, font_weight=\"bold\", arrows=True, arrowstyle=\"->\", arrowsize=15)\n",
    "plt.title(\"A conceptual diagram illustrating the relationship between AI-related factors and user trust/adoption\", fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from semopy import Model, Optimizer\n",
    "\n",
    "# Parameters\n",
    "p_value_threshold = 0.05  # Set the threshold for p-value\n",
    "dependent_variable = 'Yvar_USE_AI_Work'  # Set the dependent variable\n",
    "segment_results = 'N'  # 'Y' or 'N'\n",
    "segmentation_column = 'Generation'  # The column used for segmentation if segment_results = 'Y'\n",
    "filter_column = None  # The column used for filtering\n",
    "filter_values = [None]  # List of values to exclude from the dataset\n",
    "\n",
    "# Define the path to your dataset\n",
    "current_directory = os.getcwd()\n",
    "excel_path = os.path.join(current_directory, '01-data', 'TAM_DEF.xlsx')\n",
    "summary_dir = os.path.join(current_directory, '04-summary')\n",
    "\n",
    "# Ensure the summary directory exists\n",
    "if not os.path.exists(summary_dir):\n",
    "    os.makedirs(summary_dir)\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found. Please check the file path: {excel_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error during dataset loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Apply filtering if filter_column and filter_values are set\n",
    "if filter_column and filter_values:\n",
    "    df = df[~df[filter_column].isin(filter_values)]\n",
    "    print(f\"Filtered dataset to exclude {filter_values} in {filter_column}\")\n",
    "\n",
    "if segment_results == 'Y':\n",
    "    # Get unique values in the segmentation column\n",
    "    segments = df[segmentation_column].unique()\n",
    "    print(f\"Found {segmentation_column} segments: {segments}\")\n",
    "else:\n",
    "    # If not segmenting, treat the entire dataset as a single segment\n",
    "    segments = ['Entire Dataset']\n",
    "    df['Entire Dataset'] = 'Entire Dataset'  # Add a dummy column to facilitate the loop\n",
    "\n",
    "# Initialize a list to store hypothesis results for all segments\n",
    "all_hypothesis_results = []\n",
    "\n",
    "# Loop through each segment and run the SEM model\n",
    "for segment in segments:\n",
    "    df_segment = df[df[segmentation_column] == segment] if segment_results == 'Y' else df\n",
    "    \n",
    "    # Define the SEM model\n",
    "    model_desc = f\"\"\"\n",
    "    # Latent variables\n",
    "    Trust =~ VAR11_PRIVACY_AI_Protect_Data + VAR16_ETHICS_AI_Developed_Ethical + VAR25_FAIRNESS_AI_Treats_All_Fair + VAR26_FAIRNESS_Should_Reduce_Bias\n",
    "    Ease_of_Use =~  VAR28_PERSONAL_Enhances_Experience + VAR29_PERONAL_Improves_CS_quality\n",
    "    Training =~ VAR05_CG_AI_Training_Supported + VAR03_CG_AI_Training_Access + VAR01_CG_Training +VAR04_CG_AI_Training_helps_skills\n",
    "    \n",
    "\n",
    "    # Direct relationships with Usage ({dependent_variable})\n",
    "    {dependent_variable} ~ Trust\n",
    "    {dependent_variable} ~ Ease_of_Use\n",
    "    {dependent_variable} ~ Training\n",
    "\n",
    "    # Relationships with latent variables\n",
    "    Trust ~ Training\n",
    "    Trust ~ VAR16_ETHICS_AI_Developed_Ethical\n",
    "    Trust ~ VAR15_SAFETY_AI_protect_Cyber_Threats\n",
    "    Trust ~ VAR17_ETHICS_AI_proritize_Human_Wellbeing\n",
    "    Trust ~ VAR11_PRIVACY_AI_Protect_Data\n",
    "    Trust ~ VAR25_FAIRNESS_AI_Treats_All_Fair\n",
    "    Trust ~ VAR26_FAIRNESS_Should_Reduce_Bias\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the model and load the dataset into the model\n",
    "    try:\n",
    "        model = Model(model_desc)\n",
    "        model.load_dataset(df_segment)\n",
    "        print(f\"\\nModel created and dataset loaded into the model successfully for {segment}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model creation or dataset loading for {segment}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Optimize the model\n",
    "    try:\n",
    "        optim = Optimizer(model)\n",
    "        optim.optimize()\n",
    "        print(f\"Model optimization completed successfully for {segment}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model optimization for {segment}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Extract the results\n",
    "    try:\n",
    "        results = model.inspect()\n",
    "        # Convert any \"Not estimated\" or non-numeric values to NaN\n",
    "        results = results.applymap(lambda x: np.nan if x in [\"Not estimated\", \"-\", None] else x)\n",
    "        print(f\"\\nFull Results DataFrame for {segment}:\")\n",
    "        print(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during results extraction for {segment}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Attempt to extract p-values for the paths\n",
    "    try:\n",
    "        pvalues = results[['p-value']].apply(pd.to_numeric, errors='coerce')  # Convert to numeric, set errors to NaN\n",
    "        print(f\"\\nP-values extracted successfully for {segment}:\")\n",
    "        print(pvalues)\n",
    "    except KeyError:\n",
    "        print(f\"\\nUnable to extract p-values for {segment}. Check the results DataFrame above for available data.\")\n",
    "        continue\n",
    "\n",
    "    # Define hypotheses and their corresponding paths based on the five hypotheses\n",
    "    hypothesis_criteria = [\n",
    "        (f\"Hypothesis 1: AI Training and Career Growth Opportunities Influence Ease of Use and Trust\", f'{dependent_variable} ~ Training'),\n",
    "        (f\"Hypothesis 2: Explainability and Fairness Influence User Trust and AI Adoption\", f'{dependent_variable} ~ Trust'),\n",
    "        (f\"Hypothesis 3: Ethical Development and Social Responsibility Influence User Trust\", 'Trust ~ VAR17_ETHICS_AI_proritize_Human_Wellbeing'),\n",
    "        (f\"Hypothesis 4: AI-Driven Personalization Enhances User Experience\", f'{dependent_variable} ~ Ease_of_Use'),\n",
    "        (f\"Hypothesis 5: AI Training Accessibility and Accountability Influence User Trust and Usage\", 'Trust ~ Training')\n",
    "    ]\n",
    "\n",
    "    # Create a DataFrame to store the hypothesis results for the current segment\n",
    "    hypothesis_results = []\n",
    "\n",
    "    # Determine whether each hypothesis is accepted or rejected\n",
    "    for hyp, path in hypothesis_criteria:\n",
    "        matching_paths = results[(results['lval'] == path.split(' ~ ')[0]) & \n",
    "                                 (results['rval'] == path.split(' ~ ')[1])]\n",
    "        if not matching_paths.empty:\n",
    "            p_value = matching_paths['p-value'].values[0]\n",
    "            estimate = matching_paths['Estimate'].values[0]\n",
    "            std_err = matching_paths['Std. Err'].values[0]\n",
    "            z_value = matching_paths['z-value'].values[0]\n",
    "            result = 'Accepted' if not np.isnan(p_value) and p_value < p_value_threshold else 'Rejected'\n",
    "            hypothesis_results.append({\n",
    "                'Segment': segment,\n",
    "                'Hypothesis': hyp,\n",
    "                'p-value': p_value,\n",
    "                'Estimate': estimate,\n",
    "                'Std. Err': std_err,\n",
    "                'z-value': z_value,\n",
    "                'Result': result\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Path {path} not found in results for {segment}. Please check the available paths.\")\n",
    "            hypothesis_results.append({\n",
    "                'Segment': segment,\n",
    "                'Hypothesis': hyp,\n",
    "                'p-value': np.nan,\n",
    "                'Estimate': np.nan,\n",
    "                'Std. Err': np.nan,\n",
    "                'z-value': np.nan,\n",
    "                'Result': 'Path Not Found'\n",
    "            })\n",
    "\n",
    "    # Append the current segment's hypothesis results to the overall list\n",
    "    all_hypothesis_results.extend(hypothesis_results)\n",
    "\n",
    "# Convert the overall hypothesis results to a DataFrame\n",
    "all_hypothesis_df = pd.DataFrame(all_hypothesis_results)\n",
    "\n",
    "# Save the results to Excel\n",
    "output_path = os.path.join(summary_dir, 'SEM_Results.xlsx')\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    # Write the overall hypothesis results\n",
    "    all_hypothesis_df.to_excel(writer, sheet_name='Hypothesis Results', index=False)\n",
    "    \n",
    "    # Loop through each segment and write the SEM results for each segment in a separate tab\n",
    "    for segment in segments:\n",
    "        df_segment_results = df[df[segmentation_column] == segment] if segment_results == 'Y' else df\n",
    "        # Extract the SEM results for the segment\n",
    "        model = Model(model_desc)\n",
    "        model.load_dataset(df_segment_results)\n",
    "        optim = Optimizer(model)\n",
    "        optim.optimize()\n",
    "        results = model.inspect()\n",
    "        # Write SEM results to its own sheet\n",
    "        results.to_excel(writer, sheet_name=f'SEM Results - {segment}')\n",
    "\n",
    "# Print the final hypothesis results\n",
    "print(\"\\nFinal Hypothesis Results:\")\n",
    "print(all_hypothesis_df)\n",
    "\n",
    "print(f\"\\nSEM results and hypothesis results saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEM Bootscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from semopy import Model, Optimizer\n",
    "\n",
    "# Parameters\n",
    "p_value_threshold = 0.1  # Set the threshold for p-value\n",
    "dependent_variable = 'Yvar_USE_AI_Work'  # Set the dependent variable\n",
    "segment_results = 'N'  # 'Y' or 'N'\n",
    "segmentation_column = 'Generation'  # The column used for segmentation if segment_results = 'Y'\n",
    "filter_column = None  # The column used for filtering\n",
    "filter_values = [None]  # List of values to exclude from the dataset\n",
    "n_bootstrap = 1  # Number of bootstrap samples for testing\n",
    "\n",
    "# Define the path to your dataset\n",
    "current_directory = os.getcwd()\n",
    "excel_path = os.path.join(current_directory, '01-data', 'TAM_DEF.xlsx')\n",
    "summary_dir = os.path.join(current_directory, '04-summary')\n",
    "\n",
    "# Ensure the summary directory exists\n",
    "if not os.path.exists(summary_dir):\n",
    "    os.makedirs(summary_dir)\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found. Please check the file path: {excel_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error during dataset loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Apply filtering if filter_column and filter_values are set\n",
    "if filter_column and filter_values:\n",
    "    df = df[~df[filter_column].isin(filter_values)]\n",
    "    print(f\"Filtered dataset to exclude {filter_values} in {filter_column}\")\n",
    "\n",
    "if segment_results == 'Y':\n",
    "    # Get unique values in the segmentation column\n",
    "    segments = df[segmentation_column].unique()\n",
    "    print(f\"Found {segmentation_column} segments: {segments}\")\n",
    "else:\n",
    "    # If not segmenting, treat the entire dataset as a single segment\n",
    "    segments = ['Entire Dataset']\n",
    "    df['Entire Dataset'] = 'Entire Dataset'  # Add a dummy column to facilitate the loop\n",
    "\n",
    "# Function to run the model and get results\n",
    "def run_model_and_get_results(df_segment):\n",
    "    model_desc = f\"\"\"\n",
    "    # Latent variables\n",
    "    Trust =~ VAR11_PRIVACY_AI_Protect_Data + VAR16_ETHICS_AI_Developed_Ethical + VAR25_FAIRNESS_AI_Treats_All_Fair + VAR26_FAIRNESS_Should_Reduce_Bias\n",
    "    Ease_of_Use =~  VAR28_PERSONAL_Enhances_Experience + VAR29_PERONAL_Improves_CS_quality\n",
    "    Training =~ VAR05_CG_AI_Training_Supported + VAR03_CG_AI_Training_Access + VAR01_CG_Training +VAR04_CG_AI_Training_helps_skills\n",
    "    \n",
    "    # Direct relationships with Usage ({dependent_variable})\n",
    "    {dependent_variable} ~ Trust\n",
    "    {dependent_variable} ~ Ease_of_Use\n",
    "    {dependent_variable} ~ Training\n",
    "\n",
    "    # Relationships with latent variables\n",
    "    Trust ~ Training\n",
    "    Trust ~ VAR16_ETHICS_AI_Developed_Ethical\n",
    "    Trust ~ VAR15_SAFETY_AI_protect_Cyber_Threats\n",
    "    Trust ~ VAR17_ETHICS_AI_proritize_Human_Wellbeing\n",
    "    Trust ~ VAR11_PRIVACY_AI_Protect_Data\n",
    "    Trust ~ VAR25_FAIRNESS_AI_Treats_All_Fair\n",
    "    Trust ~ VAR26_FAIRNESS_Should_Reduce_Bias\n",
    "    \"\"\"\n",
    "    model = Model(model_desc)\n",
    "    model.load_dataset(df_segment)\n",
    "    optim = Optimizer(model)\n",
    "    optim.optimize()\n",
    "    results = model.inspect()\n",
    "    return results\n",
    "#---------------------------------\n",
    "\n",
    "# Run non-bootstrapped model and get results\n",
    "try:\n",
    "    non_bootstrap_results = run_model_and_get_results(df)\n",
    "    print(\"Non-Bootstrap Results:\")\n",
    "    #print(non_bootstrap_results)\n",
    "except Exception as e:\n",
    "    print(f\"Error during non-bootstrapped model creation: {e}\")\n",
    "\n",
    "# Initialize list to hold bootstrapped results\n",
    "bootstrap_estimates = []\n",
    "\n",
    "# Bootstrapping\n",
    "for i in range(n_bootstrap):\n",
    "    print(f\"\\nProcessing bootstrap sample {i+1}/{n_bootstrap} for segment: Entire Dataset\")\n",
    "    bootstrap_sample = df.sample(n=len(df), replace=True)\n",
    "    \n",
    "    #print(f\"Bootstrap Sample {i+1} Head:\\n\", bootstrap_sample.head())\n",
    "    \n",
    "    try:\n",
    "        bootstrap_results = run_model_and_get_results(bootstrap_sample)\n",
    "        #print(f\"Bootstrap {i+1} results:\\n\", bootstrap_results[['Estimate', 'p-value']])\n",
    "        \n",
    "        # Convert all relevant columns to numeric, coercing errors\n",
    "        bootstrap_results[['Estimate', 'Std. Err', 'z-value', 'p-value']] = bootstrap_results[['Estimate', 'Std. Err', 'z-value', 'p-value']].apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        bootstrap_estimates.append(bootstrap_results[['lval', 'rval', 'Estimate', 'Std. Err', 'z-value', 'p-value']])\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model creation or optimization for bootstrap sample {i+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Aggregating bootstrapped results\n",
    "if bootstrap_estimates:\n",
    "    bootstrap_df = pd.concat(bootstrap_estimates)\n",
    "    bootstrap_df = bootstrap_df.groupby(['lval', 'rval']).mean(numeric_only=True)\n",
    "\n",
    "    # Prepare final results for hypothesis testing\n",
    "    hypothesis_results = []\n",
    "    hypothesis_criteria = {\n",
    "        'H 1: AI Training and Career Growth Opportunities Influence Ease of Use and Trust': (dependent_variable, 'Training'),\n",
    "        'H 2: Explainability and Fairness Influence User Trust and AI Adoption': (dependent_variable, 'Trust'),\n",
    "        'H 3: Ethical Development and Social Responsibility Influence User Trust': ('Trust', 'VAR17_ETHICS_AI_proritize_Human_Wellbeing'),\n",
    "        'H 4: AI-Driven Personalization Enhances User Experience': (dependent_variable, 'Ease_of_Use'),\n",
    "        'H 5: AI Training Accessibility and Accountability Influence User Trust and Usage': ('Trust', 'Training')\n",
    "    }\n",
    "\n",
    "    for hypothesis, (lval, rval) in hypothesis_criteria.items():\n",
    "        if (lval, rval) in bootstrap_df.index:\n",
    "            p_value = bootstrap_df.loc[(lval, rval), 'p-value']\n",
    "            result = 'Accepted' if p_value < p_value_threshold else 'Rejected'\n",
    "            hypothesis_results.append({\n",
    "                'Hypothesis': hypothesis,\n",
    "                'p-value': p_value,\n",
    "                'Estimate': bootstrap_df.loc[(lval, rval), 'Estimate'],\n",
    "                'Std. Err': bootstrap_df.loc[(lval, rval), 'Std. Err'],\n",
    "                'z-value': bootstrap_df.loc[(lval, rval), 'z-value'],\n",
    "                'Result': result\n",
    "            })\n",
    "        else:\n",
    "            hypothesis_results.append({\n",
    "                'Hypothesis': hypothesis,\n",
    "                'p-value': np.nan,\n",
    "                'Estimate': np.nan,\n",
    "                'Std. Err': np.nan,\n",
    "                'z-value': np.nan,\n",
    "                'Result': 'Error'\n",
    "            })\n",
    "\n",
    "    # Convert hypothesis results to a DataFrame\n",
    "    hypothesis_results_df = pd.DataFrame(hypothesis_results)\n",
    "\n",
    "    print(\"\\nHypothesis Testing Results:\")\n",
    "    print(hypothesis_results_df)\n",
    "\n",
    "    # Save the results to Excel\n",
    "    output_path = os.path.join(summary_dir, 'SEM_Results_Bootstrap_Final.xlsx')\n",
    "    with pd.ExcelWriter(output_path) as writer:\n",
    "        # Save non-bootstrapped results\n",
    "        non_bootstrap_results.to_excel(writer, sheet_name='Non-Bootstrap Results', index=False)\n",
    "        \n",
    "        # Save aggregated bootstrap results\n",
    "        bootstrap_df.to_excel(writer, sheet_name='Aggregated Bootstrap Results')\n",
    "        \n",
    "        # Save hypothesis testing results\n",
    "        hypothesis_results_df.to_excel(writer, sheet_name='Hypothesis Testing Results', index=False)\n",
    "        \n",
    "        print(f\"\\nFinal results saved to {output_path}.\")\n",
    "else:\n",
    "    print(\"No valid bootstrap estimates were generated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
